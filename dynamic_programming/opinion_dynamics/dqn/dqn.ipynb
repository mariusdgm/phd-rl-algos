{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import traceback\n",
    "from typing import Dict\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 23:17:55,868 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan14-225523_configs_estimator.args_.lin_hidden_out_size=128; estimator.args_.conv_hidden_out_size=32\n",
      "2025-01-14 23:17:55,868 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan14-225523_configs_estimator.args_.lin_hidden_out_size=128; estimator.args_.conv_hidden_out_size=32\n",
      "2025-01-14 23:17:55,868 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan14-225523_configs_estimator.args_.lin_hidden_out_size=128; estimator.args_.conv_hidden_out_size=32\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestart_training_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m     26\u001b[0m     path_previous_experiments_outputs \u001b[38;5;241m=\u001b[39m create_path_to_experiment_folder(\n\u001b[0;32m     27\u001b[0m         config,\n\u001b[0;32m     28\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     29\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestart_training_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     30\u001b[0m     )\n\u001b[1;32m---> 32\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgentDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_output_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_previous_experiments_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m experiment_agent\u001b[38;5;241m.\u001b[39mtrain(train_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs_to_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dynamic_programming\\opinion_dynamics\\dqn\\opinion_dqn.py:99\u001b[0m, in \u001b[0;36mAgentDQN.__init__\u001b[1;34m(self, train_env, validation_env, experiment_output_folder, experiment_name, resume_training_path, save_checkpoints, logger, config)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m replace_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_models(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)  \u001b[38;5;66;03m# init policy, target and optim\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Set initial values related to training and monitoring\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dynamic_programming\\opinion_dynamics\\dqn\\opinion_dqn.py:207\u001b[0m, in \u001b[0;36mAgentDQN._load_config_settings\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    197\u001b[0m eps_settings \u001b[38;5;241m=\u001b[39m agent_params\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m250_000\u001b[39m}\n\u001b[0;32m    199\u001b[0m )\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_by_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_linear_decay_function(\n\u001b[0;32m    201\u001b[0m     start\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    202\u001b[0m     end\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    203\u001b[0m     decay\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    204\u001b[0m     eps_decay_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_start_size,\n\u001b[0;32m    205\u001b[0m )\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_and_init_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# sets up in_features etc...\u001b[39;00m\n\u001b[0;32m    209\u001b[0m buffer_settings \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100_000\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m    211\u001b[0m )\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\n\u001b[0;32m    213\u001b[0m     max_size\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100_000\u001b[39m),\n\u001b[0;32m    214\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m    215\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    216\u001b[0m     n_step\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    217\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dynamic_programming\\opinion_dynamics\\dqn\\opinion_dqn.py:300\u001b[0m, in \u001b[0;36mAgentDQN._read_and_init_envs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m state_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# permute to get batch, channel, w, h shape\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;66;03m# specific to minatar\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m (\u001b[43mstate_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, state_shape[\u001b[38;5;241m0\u001b[39m], state_shape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "yaml_path = r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dynamic_programming\\opinion_dynamics\\dqn\\experiments\\results\\2025Jan14-225523_configs\\0000_estimator.args_.lin_hidden_out_size_128__estimator.args_.conv_hidden_out_size_32\\0\\cfg.yaml\"\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
