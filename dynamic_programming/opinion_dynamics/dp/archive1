import numpy as np
from scipy.linalg import expm
from scipy.interpolate import interp1d


def optimal_control_action(env, total_budget):
    """
    Implement the optimal control strategy using precomputed centralities and influence powers.

    Args:
        env (NetworkGraph): The NetworkGraph environment.
        total_budget (int): The total number of units of max_u to spend.

    Returns:
        tuple: (control input vector to be applied, remaining budget, indices of controlled agents)
    """
    centralities = env.centralities.astype(np.float64)
    opinions = env.opinions.astype(np.float64)
    desired_opinion = np.float64(env.desired_opinion)

    # Calculate influence power for each agent
    influence_powers = centralities * np.abs(desired_opinion - opinions)

    # Sort agents based on influence power in descending order
    sorted_indices = np.argsort(influence_powers)[::-1]

    # Initialize control inputs
    u = np.zeros(env.num_agents, dtype=np.float64)
    remaining_budget = np.float64(
        total_budget * env.max_u
    )  # Total control units available

    controlled_agents = []

    epsilon = np.float64(1e-8)  # Threshold for floating-point precision errors

    for idx in sorted_indices:
        if remaining_budget < epsilon:
            break  # Remaining budget is effectively zero
        control_input = min(env.max_u, remaining_budget)
        u[idx] = control_input
        remaining_budget -= control_input
        # Correct any negative remaining budget due to floating-point errors
        if remaining_budget < 0:
            remaining_budget = np.float64(0.0)
        controlled_agents.append(idx)

    return u, remaining_budget, controlled_agents


# def state_transition(env, x, u, step_duration):
#     # Ensure x and u are of type np.float64
#     x = np.float64(x)
#     u = u.astype(np.float64)

#     # Compute the controlled opinions
#     opinions_controlled = u * env.desired_opinion + (1 - u) * x

#     # Propagate opinions over the network
#     expL = expm(-env.L * step_duration)
#     opinions_next = expL @ opinions_controlled

#     return np.mean(opinions_next)

def state_transition(env, x, u, step_duration):
    # Simplify the state transition assuming the network reaches consensus
    xplus = np.sum(env.centralities * (u * env.desired_opinion + (1 - u) * x))
    return xplus

def run_initial_campaign(env, rem, step_duration):
    N = env.num_agents
    d = env.desired_opinion
    ubar = env.max_u
    x0 = np.mean(env.opinions)
    
    # Compute initial influence powers
    deviations = np.abs(env.opinions - d)
    influence_powers = env.centralities * deviations
    order0 = np.argsort(influence_powers)[::-1]
    
    # Determine the optimal control action for the initial campaign
    val = np.inf
    u0 = np.zeros(N, dtype=np.float64)
    best_beta0 = 0
    x1 = x0
    for beta0 in range(0, min(rem, N) + 1):
        u = np.zeros(N, dtype=np.float64)
        if beta0 > 0:
            u[order0[beta0 - 1]] = ubar
        xplus = state_transition(env, x0, u, step_duration)
        cost = np.abs(xplus - d)
        if cost < val:
            val = cost
            u0 = u.copy()
            x1 = xplus
            best_beta0 = beta0
    rem -= best_beta0
    return u0, x1, rem, best_beta0


def dynamic_programming_strategy(env, M, Q, step_duration):
    N = env.num_agents
    d = np.float64(env.desired_opinion)
    ubar = np.float64(env.max_u)
    x0 = np.float64(np.mean(env.opinions))

    nx = 10  # Increased number of discretization points
    Xgrid = np.linspace(0, 1, nx, dtype=np.float64)

    # Compute initial influence powers for the first iteration
    deviations = np.abs(env.opinions - d)
    influence_powers = env.centralities * deviations
    order0 = np.argsort(influence_powers)[::-1]

    # Agent order based on centrality for subsequent iterations
    order = np.argsort(env.centralities)[::-1]

    # Initialize value function V
    V = np.full((M + 1, nx, Q + 1), np.inf, dtype=np.float64)
    # Final cost at stage M
    for ix, x in enumerate(Xgrid):
        V[M, ix, :] = np.abs(x - d)

    # Initialize policy
    policy = [{} for _ in range(M)]

    # Backward induction
    for k in range(M - 1, 0, -1):
        print(f"DP step {k + 1}")
        for ix, x in enumerate(Xgrid):
            x = np.float64(x)
            for rem in range(Q + 1):
                val = np.inf
                best_policy_entry = None

                for beta in range(0, min(rem, N) + 1):
                    u = np.zeros(N, dtype=np.float64)
                    if beta > 0:
                        if k == 0:  # First iteration
                            u[order0[:beta]] = ubar
                        else:
                            u[order[:beta]] = ubar

                    xplus = state_transition(env, x, u, step_duration)
                    xplus = np.clip(xplus, Xgrid[0], Xgrid[-1])  # Clamp xplus
                    remplus = rem - beta

                    if remplus >= 0:
                        # Interpolate future cost
                        future_cost_function = interp1d(
                            Xgrid,
                            V[k + 1, :, remplus],
                            kind="cubic",  # Use cubic interpolation
                            fill_value="extrapolate",
                            assume_sorted=True,
                            bounds_error=False,
                        )
                        vplus = future_cost_function(xplus)

                        total_cost = vplus

                        if total_cost < val:
                            val = total_cost
                            best_policy_entry = {
                                "beta": beta,
                                "u": u.copy(),
                                "xplus": xplus,
                                "remplus": remplus,
                                "controlled_agents": (
                                    order0[:beta] if k == 0 else order[:beta]
                                ),
                            }

                V[k, ix, rem] = val
                if best_policy_entry is not None:
                    policy_key = (ix, rem)
                    policy[k][policy_key] = best_policy_entry

    return policy, Xgrid, V


def extract_optimal_policy(policy, env, Xgrid, V, M, Q, step_duration, x1, rem):
    N = env.num_agents
    d = env.desired_opinion
    ubar = env.max_u
    X = np.zeros(M + 1, dtype=np.float64)
    X[0] = x1  # Start from x1 after initial campaign
    optimal_budget_allocation = []
    nodes_controlled = []
    control_inputs = []

    # Adjust indices since we start from stage 1
    for k in range(1, M):
        x = X[k - 1]
        ix = np.argmin(np.abs(Xgrid - x))
        rem_budget = rem
        found = False
        # Try exact index
        policy_entry = policy[k].get((ix, rem_budget))
        if policy_entry is None:
            # Try adjusting ix by checking nearby indices
            for delta in [-1, 1]:
                ix_try = ix + delta
                if 0 <= ix_try < len(Xgrid):
                    policy_entry = policy[k].get((ix_try, rem_budget))
                    if policy_entry is not None:
                        ix = ix_try
                        found = True
                        break
            if not found:
                raise ValueError(
                    f"No policy entry found for state index {ix} and remaining budget {rem_budget} at stage {k}"
                )

        beta = policy_entry["beta"]
        u = policy_entry["u"]
        xplus = policy_entry["xplus"]
        rem = policy_entry["remplus"]
        controlled_agents = policy_entry["controlled_agents"]

        # Update state and budget
        X[k + 1] = xplus

        # Store the control action
        control_inputs.append(u)
        optimal_budget_allocation.append(beta)
        nodes_controlled.append(controlled_agents)

    final_opinion_error = np.abs(X[-1] - d)
    return (
        optimal_budget_allocation,
        nodes_controlled,
        control_inputs,
        final_opinion_error,
        X,
    )


def run_dynamic_programming_campaigns(
    env,
    optimal_budget_allocation,
    step_duration,
    sampling_time,
    final_campaign_step_duration=None,
    final_campaign_sampling_time=None,
):
    M = len(optimal_budget_allocation)  # Number of campaigns
    opinions_over_time = []
    time_points = []
    current_time = 0.0
    nodes_controlled_simulation = []

    for k in range(M):
        # Determine the budget for this campaign
        budget_k = optimal_budget_allocation[k]

        # Compute influence powers and agent ordering based on current opinions
        deviations = np.abs(env.opinions - env.desired_opinion)
        influence_powers = env.centralities * deviations
        agent_order = (
            np.argsort(influence_powers)[::-1]
            if k == 0
            else np.argsort(env.centralities)[::-1]
        )

        # Determine the control action using the allocated budget and agent order
        action = np.zeros(env.num_agents, dtype=np.float64)
        if budget_k > 0:
            action[agent_order[:budget_k]] = env.max_u
        controlled_nodes = np.where(action == env.max_u)[0]
        nodes_controlled_simulation.append(controlled_nodes)

        # Determine the step_duration and sampling_time for this campaign
        if k == M - 1 and final_campaign_step_duration is not None:
            campaign_step_duration = final_campaign_step_duration
            campaign_sampling_time = final_campaign_sampling_time or sampling_time
        else:
            campaign_step_duration = step_duration
            campaign_sampling_time = sampling_time

        # Run the campaign with sampling
        opinions_campaign, times_campaign = run_campaign_with_sampling(
            env,
            action=action,
            step_duration=campaign_step_duration,
            sampling_time=campaign_sampling_time,
        )

        # Append the opinions and time points
        if len(opinions_over_time) == 0:
            opinions_over_time = opinions_campaign
            time_points = times_campaign + current_time
        else:
            opinions_over_time = np.vstack((opinions_over_time, opinions_campaign[1:]))
            time_points = np.concatenate(
                (time_points, times_campaign[1:] + current_time)
            )

        # Update current time
        current_time += campaign_step_duration

    return opinions_over_time, time_points, nodes_controlled_simulation


def get_expected_value_from_dp(V, Xgrid, x0, total_budget):
    """
    Given the value function V, state grid Xgrid, initial state x0, and total budget,
    returns the expected value from the DP value function.

    Args:
        V (ndarray): Value function array of shape (M+1, nx, Q+1)
        Xgrid (ndarray): Discretized state grid of shape (nx,)
        x0 (float): Initial average opinion
        total_budget (int): Total budget available

    Returns:
        float: Expected value (cost-to-go) from the value function V
    """
    # Ensure x0 is of type np.float64
    x0 = np.float64(x0)

    # Find the index ix corresponding to x0 in Xgrid
    ix = np.argmin(np.abs(Xgrid - x0))
    expected_value = V[0, ix, total_budget]
    return expected_value


def compute_expected_value_for_budget_distribution(
    budget_allocation, env, M, step_duration
):
    """
    Simulate the opinion dynamics with a given budget distribution and compute the expected value.

    Args:
        budget_allocation (list): Budget allocated in each campaign.
        env (NetworkGraph): The environment instance.
        M (int): Number of campaigns.
        step_duration (float): Duration of each campaign.

    Returns:
        final_opinion_error (float): The absolute difference between the final average opinion and the desired opinion.
        total_cost (float): The sum of immediate costs across all campaigns.
        costs (list): Immediate cost at each campaign.
        X (list): Average opinion after each campaign.
    """
    N = env.num_agents
    d = env.desired_opinion
    ubar = env.max_u
    eigv = env.centralities
    eigv_normalized = eigv / np.sum(eigv)
    x0 = np.mean(env.opinions)
    uzero = np.zeros(N)
    X = np.zeros(M + 1)
    X[0] = x0
    total_cost = 0
    costs = []

    # Agent orders
    order = np.argsort(eigv)[::-1]
    order0 = np.argsort(eigv * np.abs(env.opinions - d))[::-1]
    rem = sum(budget_allocation)

    sim_env = deepcopy(env)
    sim_env.opinions = env.opinions.copy()

    for k in range(M):
        beta = budget_allocation[k]
        u = uzero.copy()
        beta_int = int(beta)  # Ensure beta is an integer number of agents
        if beta_int > 0:
            if k == 0:
                u[order0[:beta_int]] = ubar
            else:
                u[order[:beta_int]] = ubar
        # Apply control
        opinions_after_step, _, _, _, _ = sim_env.step(
            action=u,
            step_duration=step_duration,
        )
        sim_env.opinions = opinions_after_step.copy()
        xplus = np.mean(sim_env.opinions)
        # Compute immediate cost
        immediate_cost = abs(xplus - d)
        total_cost += immediate_cost
        costs.append(immediate_cost)
        X[k + 1] = xplus
    final_opinion_error = abs(X[-1] - d)
    return final_opinion_error, total_cost, costs, X