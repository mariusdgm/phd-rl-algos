{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:32:37,219 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-16 11:32:37,219 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-16 11:32:37,220 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-16 11:32:37,220 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-16 11:32:37,222 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-16 11:32:37,222 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-16 11:32:37,223 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (lin1): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (predict_A_b): Sequential(\n",
      "    (lin3): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-01-16 11:32:37,223 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (lin1): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (lin2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (predict_A_b): Sequential(\n",
      "    (lin3): Linear(in_features=32, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-01-16 11:32:37,223 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-16 11:32:37,223 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-16 11:32:37,224 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-16 11:32:37,224 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:811: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  actions = torch.FloatTensor(actions)  # Continuous actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:34:28,413 - opinion_agent_dqn - INFO - TRAINING STATS | Frames seen: 200000 | Episode: 400 | Max reward: -65.20954349138614 | Avg reward: -67.70372924618863 | Avg frames (episode): 500.0 | Avg max Q: -5715618.730975764 | Epsilon: 0.2278 | Train epoch time: 0:01:51.154937\n",
      "2025-01-16 11:34:28,413 - opinion_agent_dqn - INFO - TRAINING STATS | Frames seen: 200000 | Episode: 400 | Max reward: -65.20954349138614 | Avg reward: -67.70372924618863 | Avg frames (episode): 500.0 | Avg max Q: -5715618.730975764 | Epsilon: 0.2278 | Train epoch time: 0:01:51.154937\n",
      "2025-01-16 11:34:28,414 - opinion_agent_dqn - INFO - Starting validation epoch at t = 200000\n",
      "2025-01-16 11:34:28,414 - opinion_agent_dqn - INFO - Starting validation epoch at t = 200000\n",
      "2025-01-16 11:34:54,805 - opinion_agent_dqn - INFO - VALIDATION STATS | Max reward: -67.16648243301412 | Avg reward: -67.82058482099775 | Avg frames (episode): 499.7011952191235 | Avg max Q: -162179605.94268203 | Validation epoch time: 0:00:26.376551\n",
      "2025-01-16 11:34:54,805 - opinion_agent_dqn - INFO - VALIDATION STATS | Max reward: -67.16648243301412 | Avg reward: -67.82058482099775 | Avg frames (episode): 499.7011952191235 | Avg max Q: -162179605.94268203 | Validation epoch time: 0:00:26.376551\n",
      "2025-01-16 11:34:54,806 - opinion_agent_dqn - INFO - Saving checkpoint at t = 200000 ...\n",
      "2025-01-16 11:34:54,806 - opinion_agent_dqn - INFO - Saving checkpoint at t = 200000 ...\n",
      "2025-01-16 11:34:54,812 - opinion_agent_dqn - DEBUG - Models saved at t = 200000\n",
      "2025-01-16 11:34:54,812 - opinion_agent_dqn - DEBUG - Models saved at t = 200000\n",
      "2025-01-16 11:34:54,813 - opinion_agent_dqn - DEBUG - Training status saved at t = 200000\n",
      "2025-01-16 11:34:54,813 - opinion_agent_dqn - DEBUG - Training status saved at t = 200000\n",
      "2025-01-16 11:34:59,088 - opinion_agent_dqn - INFO - Checkpoint saved at t = 200000\n",
      "2025-01-16 11:34:59,088 - opinion_agent_dqn - INFO - Checkpoint saved at t = 200000\n",
      "2025-01-16 11:34:59,089 - opinion_agent_dqn - INFO - Epoch 0 completed in 0:02:21.865572\n",
      "2025-01-16 11:34:59,089 - opinion_agent_dqn - INFO - Epoch 0 completed in 0:02:21.865572\n",
      "2025-01-16 11:34:59,089 - opinion_agent_dqn - INFO - \n",
      "\n",
      "2025-01-16 11:34:59,089 - opinion_agent_dqn - INFO - \n",
      "\n",
      "2025-01-16 11:34:59,090 - opinion_agent_dqn - INFO - Starting training epoch at t = 200000\n",
      "2025-01-16 11:34:59,090 - opinion_agent_dqn - INFO - Starting training epoch at t = 200000\n",
      "2025-01-16 11:36:52,087 - opinion_agent_dqn - INFO - TRAINING STATS | Frames seen: 400000 | Episode: 800 | Max reward: -67.81651570870167 | Avg reward: -67.81651570870167 | Avg frames (episode): 500.0 | Avg max Q: -78583100090.67468 | Epsilon: 0.01 | Train epoch time: 0:01:52.963228\n",
      "2025-01-16 11:36:52,087 - opinion_agent_dqn - INFO - TRAINING STATS | Frames seen: 400000 | Episode: 800 | Max reward: -67.81651570870167 | Avg reward: -67.81651570870167 | Avg frames (episode): 500.0 | Avg max Q: -78583100090.67468 | Epsilon: 0.01 | Train epoch time: 0:01:52.963228\n",
      "2025-01-16 11:36:52,088 - opinion_agent_dqn - INFO - Starting validation epoch at t = 400000\n",
      "2025-01-16 11:36:52,088 - opinion_agent_dqn - INFO - Starting validation epoch at t = 400000\n",
      "2025-01-16 11:37:22,050 - opinion_agent_dqn - INFO - VALIDATION STATS | Max reward: -67.39149857310062 | Avg reward: -67.82404633099384 | Avg frames (episode): 499.7171314741036 | Avg max Q: -115272214544.1471 | Validation epoch time: 0:00:29.945666\n",
      "2025-01-16 11:37:22,050 - opinion_agent_dqn - INFO - VALIDATION STATS | Max reward: -67.39149857310062 | Avg reward: -67.82404633099384 | Avg frames (episode): 499.7171314741036 | Avg max Q: -115272214544.1471 | Validation epoch time: 0:00:29.945666\n",
      "2025-01-16 11:37:22,051 - opinion_agent_dqn - INFO - Saving checkpoint at t = 400000 ...\n",
      "2025-01-16 11:37:22,051 - opinion_agent_dqn - INFO - Saving checkpoint at t = 400000 ...\n",
      "2025-01-16 11:37:22,053 - opinion_agent_dqn - DEBUG - Models saved at t = 400000\n",
      "2025-01-16 11:37:22,053 - opinion_agent_dqn - DEBUG - Models saved at t = 400000\n",
      "2025-01-16 11:37:22,054 - opinion_agent_dqn - DEBUG - Training status saved at t = 400000\n",
      "2025-01-16 11:37:22,054 - opinion_agent_dqn - DEBUG - Training status saved at t = 400000\n",
      "2025-01-16 11:37:25,970 - opinion_agent_dqn - INFO - Checkpoint saved at t = 400000\n",
      "2025-01-16 11:37:25,970 - opinion_agent_dqn - INFO - Checkpoint saved at t = 400000\n",
      "2025-01-16 11:37:25,972 - opinion_agent_dqn - INFO - Epoch 1 completed in 0:02:26.881256\n",
      "2025-01-16 11:37:25,972 - opinion_agent_dqn - INFO - Epoch 1 completed in 0:02:26.881256\n",
      "2025-01-16 11:37:25,972 - opinion_agent_dqn - INFO - \n",
      "\n",
      "2025-01-16 11:37:25,972 - opinion_agent_dqn - INFO - \n",
      "\n",
      "2025-01-16 11:37:25,973 - opinion_agent_dqn - INFO - Ended training session after 2 epochs at t = 400000\n",
      "2025-01-16 11:37:25,973 - opinion_agent_dqn - INFO - Ended training session after 2 epochs at t = 400000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m\n\u001b[0;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m experiment_agent\u001b[38;5;241m.\u001b[39mtrain(train_epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs_to_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     53\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m my_logging\u001b[38;5;241m.\u001b[39mcleanup_file_handlers(experiment_logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'seed'"
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Jan15-202913_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
