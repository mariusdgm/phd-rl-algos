{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "import logging\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 23:37:00,591 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-29 23:37:00,592 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-29 23:37:01,395 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-29 23:37:01,396 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-29 23:37:01,396 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-29 23:37:01,397 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-29 23:37:01,887 - opinion_agent_dqn - INFO - States shape: torch.Size([32, 1, 4]), Next States shape: torch.Size([32, 1, 4])\n",
      "2025-01-29 23:37:01,888 - opinion_agent_dqn - INFO - Actions shape: torch.Size([32, 4]), Rewards shape: torch.Size([32, 1])\n",
      "2025-01-29 23:37:01,888 - opinion_agent_dqn - INFO - Dones shape: torch.Size([32, 1])\n",
      "2025-01-29 23:37:01,888 - opinion_agent_dqn - INFO - A_diag shape: torch.Size([32, 2, 4]), b shape: torch.Size([32, 2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\replay_buffer.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  actions = torch.tensor(actions, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     37\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m AgentDQN(\n\u001b[0;32m     38\u001b[0m     train_env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[0;32m     39\u001b[0m     validation_env\u001b[38;5;241m=\u001b[39mvalidation_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[43mexperiment_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m my_logging\u001b[38;5;241m.\u001b[39mcleanup_file_handlers(experiment_logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:436\u001b[0m, in \u001b[0;36mAgentDQN.train\u001b[1;34m(self, train_epochs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_epochs):\n\u001b[0;32m    434\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m--> 436\u001b[0m     ep_train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_training_epoch_info(ep_train_stats)\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats\u001b[38;5;241m.\u001b[39mappend(ep_train_stats)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:487\u001b[0m, in \u001b[0;36mAgentDQN.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    476\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epoch_t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step_cnt:\n\u001b[0;32m    478\u001b[0m     (\n\u001b[0;32m    479\u001b[0m         is_terminated,\n\u001b[0;32m    480\u001b[0m         epoch_t,\n\u001b[0;32m    481\u001b[0m         current_episode_reward,\n\u001b[0;32m    482\u001b[0m         ep_frames,\n\u001b[0;32m    483\u001b[0m         ep_policy_trained_times,\n\u001b[0;32m    484\u001b[0m         ep_target_trained_times,\n\u001b[0;32m    485\u001b[0m         ep_losses,\n\u001b[0;32m    486\u001b[0m         ep_max_qs,\n\u001b[1;32m--> 487\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_cnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m     policy_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_policy_trained_times\n\u001b[0;32m    490\u001b[0m     target_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_target_trained_times\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:551\u001b[0m, in \u001b[0;36mAgentDQN.train_episode\u001b[1;34m(self, epoch_t, train_frames)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    550\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m--> 551\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_model_update_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:815\u001b[0m, in \u001b[0;36mAgentDQN.model_learn\u001b[1;34m(self, sample, debug)\u001b[0m\n\u001b[0;32m    812\u001b[0m q_full \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mquadratic_term \u001b[38;5;241m-\u001b[39m linear_term  \u001b[38;5;66;03m# Shape: [batch_size, nr_betas]\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;66;03m# Select Q-value corresponding to the chosen beta\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m selected_q_value \u001b[38;5;241m=\u001b[39m \u001b[43mq_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, 1]\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# Forward pass through the target model for next states\u001b[39;00m\n\u001b[0;32m    818\u001b[0m next_A_diag, next_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model(next_states)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Jan15-202913_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
