{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "import logging\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-02 23:22:39,093 - opinion_agent_dqn - INFO - 1134940729.py:21 - Starting experiment: 2025Feb02-230942_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-02-02 23:22:39,094 - opinion_agent_dqn - INFO - opinion_dqn.py:222 - Loaded configuration settings.\n",
      "2025-02-02 23:22:40,148 - opinion_agent_dqn - INFO - opinion_dqn.py:281 - Initialized newtworks and optimizer.\n",
      "2025-02-02 23:22:40,149 - opinion_agent_dqn - INFO - 1134940729.py:48 - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_c): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-02-02 23:22:40,149 - opinion_agent_dqn - INFO - opinion_dqn.py:464 - Starting training session at: 0\n",
      "2025-02-02 23:22:40,150 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 0\n",
      "2025-02-02 23:22:55,518 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 20000 | Episode: 28 | Max reward: -76.67499240658502 | Avg reward: -189.85717900751973 | Avg frames (episode): 707.3214285714286 | Avg max Q: -0.33949903155202243 | Epsilon: 0.406 | Train epoch time: 0:00:15.365986\n",
      "2025-02-02 23:22:55,519 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:806: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32), epsilon=self.validation_epsilon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-02 23:22:59,792 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -131.83335717971457 | Avg reward: -131.84546686092958 | Avg frames (episode): 501.0 | Avg max Q: -0.36662109723356806 | Validation epoch time: 0:00:04.270103\n",
      "2025-02-02 23:22:59,792 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 20000 ...\n",
      "2025-02-02 23:22:59,977 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 20000\n",
      "2025-02-02 23:22:59,978 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 0 completed in 0:00:19.828510\n",
      "2025-02-02 23:22:59,978 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-02 23:22:59,978 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 20000\n",
      "2025-02-02 23:23:14,362 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 40000 | Episode: 63 | Max reward: -111.59336503641997 | Avg reward: -179.83963688019605 | Avg frames (episode): 520.1428571428571 | Avg max Q: -0.4618571807963217 | Epsilon: 0.01 | Train epoch time: 0:00:14.379507\n",
      "2025-02-02 23:23:14,363 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m\n\u001b[0;32m     37\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m AgentDQN(\n\u001b[0;32m     38\u001b[0m     train_env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[0;32m     39\u001b[0m     validation_env\u001b[38;5;241m=\u001b[39mvalidation_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     50\u001b[0m )\n\u001b[1;32m---> 52\u001b[0m \u001b[43mexperiment_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m my_logging\u001b[38;5;241m.\u001b[39mcleanup_file_handlers(experiment_logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:480\u001b[0m, in \u001b[0;36mAgentDQN.train\u001b[1;34m(self, train_epochs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats\u001b[38;5;241m.\u001b[39mappend(ep_train_stats)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_enabled:\n\u001b[1;32m--> 480\u001b[0m     ep_validation_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_validation_epoch_info(ep_validation_stats)\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_stats\u001b[38;5;241m.\u001b[39mappend(ep_validation_stats)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:738\u001b[0m, in \u001b[0;36mAgentDQN.validate_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m valiation_t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step_cnt:\n\u001b[0;32m    734\u001b[0m     (\n\u001b[0;32m    735\u001b[0m         current_episode_reward,\n\u001b[0;32m    736\u001b[0m         ep_frames,\n\u001b[0;32m    737\u001b[0m         ep_max_qs,\n\u001b[1;32m--> 738\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    740\u001b[0m     valiation_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_frames\n\u001b[0;32m    742\u001b[0m     epoch_episode_rewards\u001b[38;5;241m.\u001b[39mappend(current_episode_reward)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:805\u001b[0m, in \u001b[0;36mAgentDQN.validate_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    803\u001b[0m is_terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminated:\n\u001b[1;32m--> 805\u001b[0m     action, max_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_epsilon\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)\n\u001b[0;32m    809\u001b[0m     s_prime, reward, is_terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_env\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m    810\u001b[0m         action\n\u001b[0;32m    811\u001b[0m     )\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:437\u001b[0m, in \u001b[0;36mAgentDQN.select_action\u001b[1;34m(self, state, epsilon, random_action)\u001b[0m\n\u001b[0;32m    434\u001b[0m max_q, beta_idx \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    435\u001b[0m optimal_w \u001b[38;5;241m=\u001b[39m w_star[torch\u001b[38;5;241m.\u001b[39marange(w_star\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), beta_idx]\n\u001b[0;32m    436\u001b[0m betas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m--> 437\u001b[0m     \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbeta_idx\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    438\u001b[0m )\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_action \u001b[38;5;129;01mor\u001b[39;00m (epsilon \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m epsilon):\n\u001b[0;32m    441\u001b[0m     noise_amplitude \u001b[38;5;241m=\u001b[39m epsilon \u001b[38;5;28;01mif\u001b[39;00m epsilon \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\_tensor.py:1099\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1096\u001b[0m         )\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1099\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;66;03m# save us work, and also helps keep trace ordering deterministic\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;66;03m# (e.g., if you zip(*hiddens), the eager map will force all the\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# indexes of hiddens[0] before hiddens[1], while the generator\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# map will interleave them.)\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Feb02-230942_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "A_b_c_net:\n",
      " tensor([[[10., 11., 12., 13., 14., 15., 16.],\n",
      "         [20., 21., 22., 23., 24., 25., 26.]],\n",
      "\n",
      "        [[30., 31., 32., 33., 34., 35., 36.],\n",
      "         [40., 41., 42., 43., 44., 45., 46.]]])\n",
      "\n",
      "Extracted c (free term):\n",
      " tensor([[10., 20.],\n",
      "        [30., 40.]])\n",
      "\n",
      "Extracted A_diag (exponentiated for positivity):\n",
      " tensor([[[5.9874e+04, 1.6275e+05, 4.4241e+05],\n",
      "         [1.3188e+09, 3.5849e+09, 9.7448e+09]],\n",
      "\n",
      "        [[2.9049e+13, 7.8963e+13, 2.1464e+14],\n",
      "         [6.3984e+17, 1.7393e+18, 4.7278e+18]]])\n",
      "\n",
      "Extracted b (bias term):\n",
      " tensor([[[14., 15., 16.],\n",
      "         [24., 25., 26.]],\n",
      "\n",
      "        [[34., 35., 36.],\n",
      "         [44., 45., 46.]]])\n"
     ]
    }
   ],
   "source": [
    "# import torch \n",
    "\n",
    "# nr_betas = 2\n",
    "# nr_agents = 3\n",
    "# batch_size = 2\n",
    "\n",
    "# # Create a sample A_b_c_net matrix with unique integer values for visualization\n",
    "# A_b_c_net = torch.tensor([\n",
    "#     [[10, 11, 12, 13, 14, 15, 16], \n",
    "#      [20, 21, 22, 23, 24, 25, 26]],  # Batch 1\n",
    "#     [[30, 31, 32, 33, 34, 35, 36], \n",
    "#      [40, 41, 42, 43, 44, 45, 46]]   # Batch 2\n",
    "# ], dtype=torch.float32)\n",
    "\n",
    "# # Extract components\n",
    "# c = A_b_c_net[:, :, 0]  # Free term (first column)\n",
    "# A_diag = torch.exp(A_b_c_net[:, :, 1 : nr_agents + 1])  # Positive definite diagonal (next `nr_agents` columns)\n",
    "# b = A_b_c_net[:, :, nr_agents + 1 :]  # Bias term (remaining columns)\n",
    "\n",
    "# # Print extracted values for visualization\n",
    "# print(nr_betas * (2 * nr_agents + 1))\n",
    "# print(\"A_b_c_net:\\n\", A_b_c_net)\n",
    "# print(\"\\nExtracted c (free term):\\n\", c)\n",
    "# print(\"\\nExtracted A_diag (exponentiated for positivity):\\n\", A_diag)\n",
    "# print(\"\\nExtracted b (bias term):\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
