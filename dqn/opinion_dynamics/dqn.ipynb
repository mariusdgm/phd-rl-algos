{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,897 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,903 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-23 10:53:01,909 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "Initial state shape: (4,)\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,917 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=36, bias=True)\n",
      ")\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,924 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-23 10:53:01,931 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "5005\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5009\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5013\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5017\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5021\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "Initial state shape: (4,)\n",
      "5025\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5029\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5033\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5037\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5041\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5045\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5049\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5053\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5057\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5061\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5065\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5069\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5073\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5077\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5081\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "q_vals shape: torch.Size([1, 4])\n",
      "A_diag shape: torch.Size([1, 4, 4])\n",
      "b shape: torch.Size([1, 4, 4])\n",
      "5085\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5089\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5093\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5097\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n",
      "5101\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "q_vals shape: torch.Size([32, 4])\n",
      "A_diag shape: torch.Size([32, 4, 4])\n",
      "b shape: torch.Size([32, 4, 4])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 3 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 51\u001b[0m\n\u001b[0;32m     36\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m AgentDQN(\n\u001b[0;32m     37\u001b[0m     train_env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[0;32m     38\u001b[0m     validation_env\u001b[38;5;241m=\u001b[39mvalidation_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[1;32m---> 51\u001b[0m \u001b[43mexperiment_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m my_logging\u001b[38;5;241m.\u001b[39mcleanup_file_handlers(experiment_logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:453\u001b[0m, in \u001b[0;36mAgentDQN.train\u001b[1;34m(self, train_epochs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_epochs):\n\u001b[0;32m    451\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m--> 453\u001b[0m     ep_train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_training_epoch_info(ep_train_stats)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats\u001b[38;5;241m.\u001b[39mappend(ep_train_stats)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:504\u001b[0m, in \u001b[0;36mAgentDQN.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    493\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epoch_t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step_cnt:\n\u001b[0;32m    495\u001b[0m     (\n\u001b[0;32m    496\u001b[0m         is_terminated,\n\u001b[0;32m    497\u001b[0m         epoch_t,\n\u001b[0;32m    498\u001b[0m         current_episode_reward,\n\u001b[0;32m    499\u001b[0m         ep_frames,\n\u001b[0;32m    500\u001b[0m         ep_policy_trained_times,\n\u001b[0;32m    501\u001b[0m         ep_target_trained_times,\n\u001b[0;32m    502\u001b[0m         ep_losses,\n\u001b[0;32m    503\u001b[0m         ep_max_qs,\n\u001b[1;32m--> 504\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_cnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m     policy_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_policy_trained_times\n\u001b[0;32m    507\u001b[0m     target_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_target_trained_times\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:565\u001b[0m, in \u001b[0;36mAgentDQN.train_episode\u001b[1;34m(self, epoch_t, train_frames)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_start_size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 565\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m         loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_learn(sample)\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\replay_buffer.py:63\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m next_states \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     59\u001b[0m     ns \u001b[38;5;28;01mif\u001b[39;00m ns\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mexpand_dims(ns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m next_states\n\u001b[0;32m     60\u001b[0m ]\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Stack states and next_states for consistent batching\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     64\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     65\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate(next_states, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     67\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 3 dimension(s)"
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Jan15-202913_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
