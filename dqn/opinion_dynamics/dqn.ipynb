{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "import logging\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 10:22:28,811 - opinion_agent_dqn - INFO - 1719336276.py:21 - Starting experiment: 2025Feb03-102156_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-02-03 10:22:28,812 - opinion_agent_dqn - INFO - opinion_dqn.py:222 - Loaded configuration settings.\n",
      "2025-02-03 10:22:29,606 - opinion_agent_dqn - INFO - opinion_dqn.py:281 - Initialized newtworks and optimizer.\n",
      "2025-02-03 10:22:29,607 - opinion_agent_dqn - INFO - 1719336276.py:48 - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_c): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-02-03 10:22:29,608 - opinion_agent_dqn - INFO - opinion_dqn.py:464 - Starting training session at: 0\n",
      "2025-02-03 10:22:29,608 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 0\n",
      "2025-02-03 10:24:41,534 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 200000 | Episode: 79 | Max reward: -75.8801865113958 | Avg reward: -4053.778842406915 | Avg frames (episode): 2490.1392405063293 | Avg max Q: -3.1557502861746514 | Epsilon: 0.2278 | Train epoch time: 0:02:11.908424\n",
      "2025-02-03 10:24:41,535 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:806: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32), epsilon=self.validation_epsilon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-03 10:25:20,797 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -256060.9396709756 | Avg reward: -256060.9396709756 | Avg frames (episode): 125000.0 | Avg max Q: -3.4693435276348645 | Validation epoch time: 0:00:39.245168\n",
      "2025-02-03 10:25:20,798 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 200000 ...\n",
      "2025-02-03 10:25:21,654 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 200000\n",
      "2025-02-03 10:25:21,654 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 0 completed in 0:02:52.046628\n",
      "2025-02-03 10:25:21,655 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-03 10:25:21,655 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 200000\n",
      "2025-02-03 10:27:34,399 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 400000 | Episode: 103 | Max reward: -167.4686414544203 | Avg reward: -13284.74813441887 | Avg frames (episode): 6907.666666666667 | Avg max Q: -3.430979286132226 | Epsilon: 0.01 | Train epoch time: 0:02:12.715942\n",
      "2025-02-03 10:27:34,400 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 400000\n",
      "2025-02-03 10:28:13,959 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -256060.9396709756 | Avg reward: -256060.9396709756 | Avg frames (episode): 125000.0 | Avg max Q: -3.8073819321749074 | Validation epoch time: 0:00:39.542572\n",
      "2025-02-03 10:28:13,960 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 400000 ...\n",
      "2025-02-03 10:28:14,902 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 400000\n",
      "2025-02-03 10:28:14,903 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 1 completed in 0:02:53.247599\n",
      "2025-02-03 10:28:14,903 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-03 10:28:14,903 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 400000\n",
      "2025-02-03 10:30:29,703 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 600000 | Episode: 158 | Max reward: -165.39698372664003 | Avg reward: -7888.933893404983 | Avg frames (episode): 4273.309090909091 | Avg max Q: -3.666238021259296 | Epsilon: 0.01 | Train epoch time: 0:02:14.754486\n",
      "2025-02-03 10:30:29,704 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 600000\n",
      "2025-02-03 10:31:12,100 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -256060.9396709756 | Avg reward: -256060.9396709756 | Avg frames (episode): 125000.0 | Avg max Q: -4.533457953896927 | Validation epoch time: 0:00:42.380282\n",
      "2025-02-03 10:31:12,100 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 600000 ...\n",
      "2025-02-03 10:31:12,936 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 600000\n",
      "2025-02-03 10:31:12,936 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 2 completed in 0:02:58.033144\n",
      "2025-02-03 10:31:12,937 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-03 10:31:12,937 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 600000\n",
      "2025-02-03 10:33:25,275 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 800000 | Episode: 281 | Max reward: -165.37636743862464 | Avg reward: -2494.653413092023 | Avg frames (episode): 1640.0 | Avg max Q: -4.805854834634382 | Epsilon: 0.01 | Train epoch time: 0:02:12.302898\n",
      "2025-02-03 10:33:25,276 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 800000\n",
      "2025-02-03 10:34:05,119 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -256060.9396709756 | Avg reward: -256060.9396709756 | Avg frames (episode): 125000.0 | Avg max Q: -7.744599777749135 | Validation epoch time: 0:00:39.827409\n",
      "2025-02-03 10:34:05,120 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 800000 ...\n",
      "2025-02-03 10:34:05,964 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 800000\n",
      "2025-02-03 10:34:05,964 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 3 completed in 0:02:53.026685\n",
      "2025-02-03 10:34:05,964 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-03 10:34:05,965 - opinion_agent_dqn - INFO - opinion_dqn.py:505 - Starting training epoch at t = 800000\n",
      "2025-02-03 10:36:19,050 - opinion_agent_dqn - INFO - opinion_dqn.py:642 - TRAINING STATS | Frames seen: 1000000 | Episode: 446 | Max reward: -162.58183526896772 | Avg reward: -1622.5062173709964 | Avg frames (episode): 1214.3030303030303 | Avg max Q: -12.169429850799872 | Epsilon: 0.01 | Train epoch time: 0:02:13.050149\n",
      "2025-02-03 10:36:19,051 - opinion_agent_dqn - INFO - opinion_dqn.py:724 - Starting validation epoch at t = 1000000\n",
      "2025-02-03 10:36:58,194 - opinion_agent_dqn - INFO - opinion_dqn.py:822 - VALIDATION STATS | Max reward: -167.56595559029432 | Avg reward: -167.5886461858023 | Avg frames (episode): 504.0 | Avg max Q: -22.130797586784425 | Validation epoch time: 0:00:39.127075\n",
      "2025-02-03 10:36:58,194 - opinion_agent_dqn - INFO - opinion_dqn.py:310 - Saving checkpoint at t = 1000000 ...\n",
      "2025-02-03 10:36:59,028 - opinion_agent_dqn - INFO - opinion_dqn.py:314 - Checkpoint saved at t = 1000000\n",
      "2025-02-03 10:36:59,029 - opinion_agent_dqn - INFO - opinion_dqn.py:490 - Epoch 4 completed in 0:02:53.064273\n",
      "2025-02-03 10:36:59,029 - opinion_agent_dqn - INFO - opinion_dqn.py:491 - \n",
      "\n",
      "2025-02-03 10:36:59,029 - opinion_agent_dqn - INFO - opinion_dqn.py:493 - Ended training session after 5 epochs at t = 1000000\n",
      "2025-02-03 10:36:59,029 - opinion_agent_dqn - INFO - 1719336276.py:54 - Finished training experiment: 2025Feb03-102156_configs_estimator.args_.lin_hidden_out_size=32, run_id: 0\n"
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Feb03-102156_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, run_id: {config[\"run_id\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "\n",
    "# nr_betas = 2\n",
    "# nr_agents = 3\n",
    "# batch_size = 2\n",
    "\n",
    "# # Create a sample A_b_c_net matrix with unique integer values for visualization\n",
    "# A_b_c_net = torch.tensor([\n",
    "#     [[10, 11, 12, 13, 14, 15, 16], \n",
    "#      [20, 21, 22, 23, 24, 25, 26]],  # Batch 1\n",
    "#     [[30, 31, 32, 33, 34, 35, 36], \n",
    "#      [40, 41, 42, 43, 44, 45, 46]]   # Batch 2\n",
    "# ], dtype=torch.float32)\n",
    "\n",
    "# # Extract components\n",
    "# c = A_b_c_net[:, :, 0]  # Free term (first column)\n",
    "# A_diag = torch.exp(A_b_c_net[:, :, 1 : nr_agents + 1])  # Positive definite diagonal (next `nr_agents` columns)\n",
    "# b = A_b_c_net[:, :, nr_agents + 1 :]  # Bias term (remaining columns)\n",
    "\n",
    "# # Print extracted values for visualization\n",
    "# print(nr_betas * (2 * nr_agents + 1))\n",
    "# print(\"A_b_c_net:\\n\", A_b_c_net)\n",
    "# print(\"\\nExtracted c (free term):\\n\", c)\n",
    "# print(\"\\nExtracted A_diag (exponentiated for positivity):\\n\", A_diag)\n",
    "# print(\"\\nExtracted b (bias term):\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
