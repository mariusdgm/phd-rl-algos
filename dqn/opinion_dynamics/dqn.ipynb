{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 2)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import Dict\n",
    "\n",
    "from liftoff import parse_opts\n",
    "\n",
    "from opinion_dqn import AgentDQN\n",
    "from utils import my_logging\n",
    "from utils.experiment import seed_everything, create_path_to_experiment_folder, build_environment\n",
    "from utils.generic import convert_namespace_to_dict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax in last layer because it scales with N instead of 2^N where N is the nr of agents\n",
    "\n",
    "# might need to do policy gradient\n",
    "\n",
    "# next step: Q iteration with action representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "train_env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,884 - opinion_agent_dqn - INFO - Starting experiment: 2025Jan15-202913_configs_estimator.args_.lin_hidden_out_size=32\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,888 - opinion_agent_dqn - INFO - Loaded configuration settings.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "2025-01-28 22:47:35,892 - opinion_agent_dqn - INFO - Initialized newtworks and optimizer.\n",
      "Initial state shape: (4,)\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,895 - opinion_agent_dqn - INFO - Initialized agent with models: OpinionNet(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (predict_A_b_q): Linear(in_features=32, out_features=18, bias=True)\n",
      ")\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,898 - opinion_agent_dqn - INFO - Starting training session at: 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "2025-01-28 22:47:35,902 - opinion_agent_dqn - INFO - Starting training epoch at t = 0\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "Initial state shape: (4,)\n",
      "5005\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5009\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5013\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5017\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "Initial state shape: (4,)\n",
      "5021\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5025\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5029\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5033\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5037\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5041\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5045\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5049\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5053\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5057\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5061\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5065\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5069\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5073\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5077\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n",
      "5081\n",
      "States: torch.Size([32, 4])\n",
      "\n",
      "A_diag shape: torch.Size([32, 2, 4])\n",
      "b shape: torch.Size([32, 2, 4])\n",
      "actions shape: torch.Size([32, 4])\n",
      "next_A_diag shape: torch.Size([32, 2, 4])\n",
      "next_b shape: torch.Size([32, 2, 4])\n",
      "selected_q_value shape: torch.Size([32, 1, 2])\n",
      "expected_q_value shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:837: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 51\u001b[0m\n\u001b[0;32m     36\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m AgentDQN(\n\u001b[0;32m     37\u001b[0m     train_env\u001b[38;5;241m=\u001b[39mtrain_env,\n\u001b[0;32m     38\u001b[0m     validation_env\u001b[38;5;241m=\u001b[39mvalidation_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialized agent with models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_agent\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[1;32m---> 51\u001b[0m \u001b[43mexperiment_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_title\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m my_logging\u001b[38;5;241m.\u001b[39mcleanup_file_handlers(experiment_logger\u001b[38;5;241m=\u001b[39mlogger)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:433\u001b[0m, in \u001b[0;36mAgentDQN.train\u001b[1;34m(self, train_epochs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_epochs):\n\u001b[0;32m    431\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m--> 433\u001b[0m     ep_train_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_training_epoch_info(ep_train_stats)\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_stats\u001b[38;5;241m.\u001b[39mappend(ep_train_stats)\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:484\u001b[0m, in \u001b[0;36mAgentDQN.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epoch_t \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step_cnt:\n\u001b[0;32m    475\u001b[0m     (\n\u001b[0;32m    476\u001b[0m         is_terminated,\n\u001b[0;32m    477\u001b[0m         epoch_t,\n\u001b[0;32m    478\u001b[0m         current_episode_reward,\n\u001b[0;32m    479\u001b[0m         ep_frames,\n\u001b[0;32m    480\u001b[0m         ep_policy_trained_times,\n\u001b[0;32m    481\u001b[0m         ep_target_trained_times,\n\u001b[0;32m    482\u001b[0m         ep_losses,\n\u001b[0;32m    483\u001b[0m         ep_max_qs,\n\u001b[1;32m--> 484\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_cnt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m     policy_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_policy_trained_times\n\u001b[0;32m    487\u001b[0m     target_trained_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ep_target_trained_times\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:534\u001b[0m, in \u001b[0;36mAgentDQN.train_episode\u001b[1;34m(self, epoch_t, train_frames)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_terminated) \u001b[38;5;129;01mand\u001b[39;00m (epoch_t \u001b[38;5;241m<\u001b[39m train_frames):\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;66;03m# Corrected call to select_action\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     action, max_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_s, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_by_frame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt))\n\u001b[1;32m--> 534\u001b[0m     s_prime, reward, is_terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m     s_prime \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(s_prime, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s_prime\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Expand to 2D if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\rl_envs_forge\\envs\\network_graph\\network_graph.py:184\u001b[0m, in \u001b[0;36mNetworkGraph.step\u001b[1;34m(self, action, step_duration)\u001b[0m\n\u001b[0;32m    181\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_u)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Use the compute_dynamics function to determine the next state\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m next_opinions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopinions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_duration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Update environment state with the new opinions\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopinions \u001b[38;5;241m=\u001b[39m next_opinions\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\rl_envs_forge\\envs\\network_graph\\network_graph.py:152\u001b[0m, in \u001b[0;36mNetworkGraph.compute_dynamics\u001b[1;34m(self, current_state, control_action, step_duration)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Compute opinion propagation with the influence of the Laplacian\u001b[39;00m\n\u001b[0;32m    151\u001b[0m expL_remaining \u001b[38;5;241m=\u001b[39m expm(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m*\u001b[39m step_duration)\n\u001b[1;32m--> 152\u001b[0m propagated_opinions \u001b[38;5;241m=\u001b[39m \u001b[43mexpL_remaining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mopinions\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Clip the resulting opinions to be within [0, 1]\u001b[39;00m\n\u001b[0;32m    155\u001b[0m new_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(propagated_opinions, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)"
     ]
    }
   ],
   "source": [
    "experiment_yaml = \"2025Jan15-202913_configs\"\n",
    "yaml_path = Path(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    ") / experiment_yaml / \"0000_estimator.args_.lin_hidden_out_size_32\" / \"0\" / \"cfg.yaml\"\n",
    "\n",
    "\n",
    "with open(yaml_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "seed = int(os.path.basename(config[\"out_dir\"]))\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "logs_file = os.path.join(config[\"out_dir\"], \"experiment_log.log\")\n",
    "\n",
    "logger = my_logging.setup_logger(\n",
    "    name=config[\"experiment\"],\n",
    "    # log_file=logs_file,\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting experiment: {config['full_title']}\")\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "path_previous_experiments_outputs = None\n",
    "if \"restart_training_timestamp\" in config:\n",
    "    path_previous_experiments_outputs = create_path_to_experiment_folder(\n",
    "        config,\n",
    "        config[\"out_dir\"],\n",
    "        config[\"restart_training_timestamp\"],\n",
    "    )\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=path_previous_experiments_outputs,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f'Initialized agent with models: {experiment_agent.policy_model}'\n",
    ")\n",
    "\n",
    "experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\n",
    "\n",
    "logger.info(\n",
    "    f'Finished training experiment: {config[\"full_title\"]}, seed: {config[\"seed\"]}'\n",
    ")\n",
    "\n",
    "my_logging.cleanup_file_handlers(experiment_logger=logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
