{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(root_dir):\n",
    "    rows = []\n",
    "\n",
    "    for name in os.listdir(root_dir):\n",
    "        experiment_path = os.path.join(root_dir, name)\n",
    "        if os.path.isdir(experiment_path):\n",
    "            for seed_name in os.listdir(experiment_path):\n",
    "                seed_path = os.path.join(experiment_path, seed_name)\n",
    "                if os.path.isdir(seed_path):\n",
    "                    row_data = process_subexperiment(\n",
    "                        seed_path, os.path.basename(root_dir)\n",
    "                    )\n",
    "                    for data in row_data:\n",
    "                        data[\"seed\"] = seed_name\n",
    "                        data[\"experiment_name\"] = name\n",
    "                    rows.extend(row_data)  \n",
    "\n",
    "    # Create a DataFrame from the rows\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_subexperiment(seed_folder_path, experiment_name):\n",
    "    cfg_data = read_config(os.path.join(seed_folder_path, \"cfg.yaml\"), experiment_name)\n",
    "    train_stats_file = find_train_stats_file(seed_folder_path)\n",
    "    if train_stats_file:\n",
    "        experiment_results = process_training_stats(\n",
    "            train_stats_file, cfg_data\n",
    "        )\n",
    "        return experiment_results\n",
    "    else:\n",
    "        return []  # Return an empty list if no train stats file is found\n",
    "\n",
    "\n",
    "def read_config(cfg_path, experiment_name):\n",
    "    with open(cfg_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        full_title = config.get(\"full_title\", \"\")\n",
    "        variable_part = remove_experiment_name(full_title, experiment_name)\n",
    "        return parse_config_variables(variable_part)\n",
    "\n",
    "\n",
    "def remove_experiment_name(full_title, experiment_name):\n",
    "    to_remove = experiment_name + \"_\"\n",
    "    return (\n",
    "        full_title[len(to_remove) :].strip()\n",
    "        if full_title.startswith(to_remove)\n",
    "        else full_title\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_config_variables(variable_str):\n",
    "    variables = {}\n",
    "    for part in variable_str.split(\";\"):\n",
    "        if \"=\" in part:\n",
    "            key, value = part.split(\"=\", 1)\n",
    "            variables[key.strip()] = value.strip()\n",
    "    return variables\n",
    "\n",
    "\n",
    "def find_train_stats_file(folder_path):\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\"_train_stats\"):\n",
    "            return os.path.join(folder_path, file)\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_training_stats(train_stats_file, cfg_data):\n",
    "    checkpoint = torch.load(train_stats_file)\n",
    "\n",
    "    training_stats = checkpoint.get(\"training_stats\", [])\n",
    "    validation_stats = checkpoint.get(\"validation_stats\", [])\n",
    "    redo_stats = checkpoint.get(\"redo_scores\", [])\n",
    "\n",
    "    stats_records = process_stats(training_stats, cfg_data, \"training\") + process_stats(\n",
    "        validation_stats, cfg_data, \"validation\"\n",
    "    )\n",
    "\n",
    "    # Combine stats records with redo scores\n",
    "    combined_records = []\n",
    "    for record in stats_records:\n",
    "        combined_record = record.copy()  # Copy the stats record\n",
    "        combined_records.append(combined_record)\n",
    "\n",
    "    return combined_records\n",
    "\n",
    "\n",
    "def process_stats(stats, cfg_data, stats_type):\n",
    "    records = []\n",
    "    for epoch_stats in stats:\n",
    "        record = {\"epoch_type\": stats_type}\n",
    "        record.update(flatten(epoch_stats))  # Flatten the epoch_stats if it's nested\n",
    "        record.update(cfg_data)  # Add configuration data\n",
    "        records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def flatten(d, parent_key=\"\", sep=\"_\"):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.abc.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['epoch_type', 'frame_stamp', 'episode_rewards_min',\n",
       "       'episode_rewards_max', 'episode_rewards_mean', 'episode_rewards_median',\n",
       "       'episode_rewards_std', 'episode_frames_min', 'episode_frames_max',\n",
       "       'episode_frames_mean', 'episode_frames_median', 'episode_frames_std',\n",
       "       'episode_losses_min', 'episode_losses_max', 'episode_losses_mean',\n",
       "       'episode_losses_median', 'episode_losses_std', 'episode_max_qs_min',\n",
       "       'episode_max_qs_max', 'episode_max_qs_mean', 'episode_max_qs_median',\n",
       "       'episode_max_qs_std', 'policy_trained_times', 'target_trained_times',\n",
       "       'epoch_time', 'estimator.args_.lin_hidden_out_size', 'seed',\n",
       "       'experiment_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_experiment(\n",
    "    r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\\2025Feb10-230339_configs\"\n",
    ")\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
