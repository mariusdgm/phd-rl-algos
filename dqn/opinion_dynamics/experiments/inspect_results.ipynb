{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 4)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dqn.opinion_dynamics.utils.experiment import process_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_sub_dir = \"2025May02-101004_configs\" # excellent run\n",
    "# experiment_sub_dir = \"2025Jul09-171538_configs\" # good latest run\n",
    "\n",
    "experiment_sub_dir = \"2025Jun23-095316_configs\" \n",
    "\n",
    "exp_path = os.path.join(os.path.abspath(\".\"), \"results\", experiment_sub_dir)\n",
    "df = process_experiment(\n",
    "    exp_path\n",
    ")\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['epoch_type'] == 'validation']\n",
    "# df[df['epoch_type'] == 'training']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter columns\n",
    "hyperparam_columns = [col for col in df.columns if \"sub_exp_cfg\" in col]\n",
    "\n",
    "# Group by hyperparameter values and compute mean episode_rewards_mean\n",
    "averaged_results = df.groupby(hyperparam_columns, as_index=False).agg({\n",
    "    'episode_rewards_mean': 'mean',  # Averaging the performance metric\n",
    "    'sub_experiment_path': 'first'   # Keep a reference to an experiment path\n",
    "})\n",
    "\n",
    "# Find the best hyperparameter set based on the highest mean reward\n",
    "best_row = averaged_results.loc[averaged_results['episode_rewards_mean'].idxmax()]\n",
    "\n",
    "# Print the best experiment path and hyperparameters\n",
    "print(f\"Best hyperparameters:\\n{best_row[hyperparam_columns]}\")\n",
    "print(f\"Best mean reward: {best_row['episode_rewards_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_metric = 'episode_rewards_mean'\n",
    "# hue_col = \"sub_exp_cfg_agent_params.args_.action_w_noise_amplitude\"\n",
    "\n",
    "# # Filter for validation episodes\n",
    "# validation_df = df[df['epoch_type'] == 'validation']\n",
    "\n",
    "# # Get the unique sorted frame_stamp values\n",
    "# unique_frames = np.sort(validation_df['frame_stamp'].unique())\n",
    "# N = 1\n",
    "# # Get all frame_stamp values after the first N unique ones\n",
    "# filtered_frames = unique_frames[N:]\n",
    "\n",
    "# # Filter the DataFrame to only include rows with these frame_stamp values\n",
    "# filtered_df = validation_df[validation_df['frame_stamp'].isin(filtered_frames)]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.lineplot(data=filtered_df, x='frame_stamp', y=selected_metric, hue=hue_col)\n",
    "# plt.ylabel(selected_metric)\n",
    "# plt.xlabel('Frame Stamp')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_metric = 'episode_discounted_rewards_mean'\n",
    "# hue_col = \"sub_exp_cfg_agent_params.args_.action_w_noise_amplitude\"\n",
    "hue_col = \"experiment_name\"\n",
    "\n",
    "validation_df = df[df['epoch_type'] == 'validation']\n",
    "# validation_df = df[df['epoch_type'] == 'training']\n",
    "\n",
    "\n",
    "# Create a new figure for each environment\n",
    "plt.figure(figsize=(10, 6))\n",
    "# sns.lineplot(data=df, x='frame_stamp', y=selected_metric, hue='model')\n",
    "sns.lineplot(data=validation_df, \n",
    "             x='frame_stamp', \n",
    "             y=selected_metric, \n",
    "             hue=hue_col,\n",
    "            #  units='seed', # Comment this to see mean + cf\n",
    "            #  estimator=None, # Comment this to see mean + cf\n",
    "             )\n",
    "\n",
    "plt.ylabel(selected_metric)\n",
    "plt.xlabel('Frame Stamp')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), title='Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "selected_metric = 'episode_frames_mean'\n",
    "hue_col = \"experiment_name\"  # or another distinguishing column like \"sub_experiment_path\"\n",
    "\n",
    "validation_df = df[df['epoch_type'] == 'validation']\n",
    "\n",
    "# Plot individual lines for each run\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=validation_df,\n",
    "    x='frame_stamp',\n",
    "    y=selected_metric,\n",
    "    hue=hue_col,\n",
    "    estimator=None,  # ← disables aggregation\n",
    "    units='sub_experiment_path',  # ← groups lines by unique run\n",
    "    lw=1,  # line width\n",
    "    alpha=0.7  # transparency\n",
    ")\n",
    "\n",
    "plt.ylabel(selected_metric)\n",
    "plt.xlabel('Frame Stamp')\n",
    "plt.title('Validation Reward Trajectories (Per Run)')\n",
    "\n",
    "# Move legend outside the plot\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), title='Experiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select data\n",
    "\n",
    "experiment_columns = [\n",
    "    'epoch_type', 'frame_stamp',\n",
    "    'episode_rewards_mean', 'episode_frames_mean', \n",
    "    'episode_discounted_rewards_mean',\n",
    "    'policy_trained_times', 'target_trained_times', 'epoch_time', \n",
    "       'seed', 'experiment_name', \"sub_experiment_path\"\n",
    "]\n",
    "hyperparam_columns = [\n",
    "   col for col in df.columns if \"sub_exp_cfg\" in col\n",
    "]\n",
    "\n",
    "cols_of_interest = experiment_columns + hyperparam_columns\n",
    "\n",
    "sub_df = df[cols_of_interest]\n",
    "sub_df = sub_df[sub_df['epoch_type'] == 'validation']\n",
    "sub_df = sub_df[sub_df['frame_stamp'] == sub_df['frame_stamp'].max()]\n",
    "\n",
    "sub_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cols = [col for col in sub_df.columns if \"sub_exp_cfg\" in col]\n",
    "aggregated_df = (\n",
    "    sub_df.groupby(agg_cols)\n",
    "    .agg(episode_discounted_rewards_mean=(\"episode_discounted_rewards_mean\", \"mean\"))\n",
    "    .reset_index()\n",
    ")\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_df[sub_df['episode_rewards_mean'] == sub_df['episode_rewards_mean'].max()]['sub_experiment_path'].values[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = 'episode_discounted_rewards_mean'\n",
    "hue_col = \"experiment_name\"\n",
    "\n",
    "# Filter for training data only\n",
    "# training_df = df[df['epoch_type'] == 'training']\n",
    "training_df = df[df['epoch_type'] == 'validation']\n",
    "\n",
    "# Step 1: Find best experiment\n",
    "experiment_means = training_df.groupby(hue_col)[selected_metric].mean()\n",
    "best_experiment = experiment_means.idxmax()\n",
    "\n",
    "# Step 2: Filter to that experiment\n",
    "best_exp_df = training_df[training_df[hue_col] == best_experiment]\n",
    "\n",
    "# Step 3: Find best seed within best experiment\n",
    "best_seed = (\n",
    "    best_exp_df.groupby('seed')[selected_metric]\n",
    "    .mean()\n",
    "    .idxmax()\n",
    ")\n",
    "\n",
    "# Step 4: Filter to best seed\n",
    "best_seed_df = best_exp_df[best_exp_df['seed'] == best_seed]\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=best_seed_df,\n",
    "    x='frame_stamp',\n",
    "    y=selected_metric,\n",
    "    lw=2,\n",
    "    label=f\"Seed {best_seed}\"\n",
    ")\n",
    "\n",
    "plt.title(f\"Best seed ({best_seed}) from best experiment: {best_experiment}\")\n",
    "plt.ylabel(selected_metric)\n",
    "plt.xlabel('Frame Stamp')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the target metric and hyperparameter columns\n",
    "target_metric = \"episode_rewards_mean\"  # Replace with actual metric column name\n",
    "# Replace with actual hyperparameter column names\n",
    "\n",
    "# Prepare data\n",
    "X = sub_df[hyperparam_columns].apply(pd.to_numeric, errors='coerce')\n",
    "y = sub_df[target_metric].fillna(3 * sub_df[target_metric].min())\n",
    "\n",
    "# Train an XGBoost model to analyze feature importance\n",
    "model = xgboost.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# SHAP analysis\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# Summary plot (feature importance)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Define readable names for selected hyperparams\n",
    "readable_names = {\n",
    "    hyperparam_columns[0]: \"noise\",\n",
    "    hyperparam_columns[1]: \"size\"\n",
    "}\n",
    "\n",
    "# Rename columns in a copy of the dataframe\n",
    "plot_df = sub_df.rename(columns=readable_names)\n",
    "\n",
    "# Assign new column names\n",
    "x_col = \"noise\"\n",
    "y_col = \"episode_rewards_mean\"\n",
    "facet_col = \"size\"\n",
    "\n",
    "# Plot\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x=x_col,\n",
    "    y=y_col,\n",
    "    color=\"experiment_name\",  # Optional: color by experiment\n",
    "    facet_col=facet_col,\n",
    "    title=\"Episode Rewards vs. Noise Faceted by Size\",\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
