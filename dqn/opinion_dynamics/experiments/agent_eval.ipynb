{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 4)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from dqn.opinion_dynamics.opinion_dqn import AgentDQN\n",
    "from dqn.opinion_dynamics.utils.my_logging import setup_logger\n",
    "from dqn.opinion_dynamics.utils.experiment import build_environment\n",
    "\n",
    "from dynamic_programming.opinion_dynamics.common.viz import plot_opinions_over_time, visualize_policy_from_env\n",
    "\n",
    "\n",
    "def instantiate_agent(exp_subdir_path: str, train_env, validation_env) -> AgentDQN:\n",
    "    \"\"\"\n",
    "    Instantiate an AgentDQN using the configuration stored in a YAML file \n",
    "    in the provided experiment subdirectory. The agent is created with the \n",
    "    given training and validation environments and loads its previous state.\n",
    "    \n",
    "    Args:\n",
    "        exp_subdir_path (str): Path to the experiment subdirectory containing the config YAML and checkpoint files.\n",
    "        train_env (gym.Env): The training environment instance.\n",
    "        validation_env (gym.Env): The validation environment instance.\n",
    "    \n",
    "    Returns:\n",
    "        AgentDQN: An instance of AgentDQN initialized using the experiment configuration and saved state.\n",
    "    \"\"\"\n",
    "    # Assume the YAML configuration is stored as 'config.yaml' in the experiment folder.\n",
    "    config_path = os.path.join(exp_subdir_path, \"cfg.yaml\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Instantiate the agent.\n",
    "    # The resume_training_path is set to the experiment folder so that the agent loads saved weights/stats.\n",
    "    agent = AgentDQN(\n",
    "        train_env=train_env,\n",
    "        validation_env=validation_env,\n",
    "        resume_training_path=exp_subdir_path,\n",
    "        experiment_name=config[\"experiment\"],\n",
    "        config=config,\n",
    "        save_checkpoints=False,  # you can set this as needed\n",
    "        logger=setup_logger(\"dqn\")\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def run_policy_agent(env, agent, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Run the simulation using the agentâ€™s policy (exploitation only).\n",
    "    \n",
    "    Args:\n",
    "        env: The environment (which must have a reset and step method).\n",
    "        agent: An already-trained AgentDQN instance.\n",
    "        max_steps: Maximum number of steps to run.\n",
    "        \n",
    "    Returns:\n",
    "        opinions_over_time: Array of opinions (states) over time.\n",
    "        time_points: Array of time stamps.\n",
    "        rewards_over_time: Array of rewards collected at each step.\n",
    "        actions_over_time: Array of actions taken at each step.\n",
    "    \"\"\"\n",
    "    time_points = []\n",
    "    rewards_over_time = []\n",
    "    actions_over_time = []  # New: record the actions used.\n",
    "    opinions_over_time = []\n",
    "    \n",
    "    current_time = 0.0\n",
    "    # Reset environment\n",
    "    state, _ = env.reset()\n",
    "    opinions_over_time.append(state.copy())\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Convert state to a batched tensor (batch size = 1)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        # Use the agent in exploitation mode (epsilon=0, random_action=False)\n",
    "        # The agent.select_action returns (action, beta_idx, q_value)\n",
    "        action, beta_idx, q_value = agent.select_action(state_tensor, epsilon=0.0, random_action=False)\n",
    "        # action is returned as a NumPy array with shape (1, n_agents)\n",
    "        action = np.squeeze(action)  # Now action has shape (n_agents,)\n",
    "        actions_over_time.append(action.copy())\n",
    "        \n",
    "        # Apply the action in the environment.\n",
    "        next_state, reward, done, truncated, _ = env.step(action, env.tau)\n",
    "        opinions_over_time.append(next_state.copy())\n",
    "        rewards_over_time.append(reward)\n",
    "        time_points.append(current_time)\n",
    "        \n",
    "        current_time += env.tau\n",
    "        state = next_state\n",
    "        \n",
    "        if done or truncated:\n",
    "            print(f\"Simulation ended at step {step}: done={done}, truncated={truncated}\")\n",
    "            break\n",
    "\n",
    "    return (np.array(opinions_over_time),\n",
    "            np.array(time_points),\n",
    "            np.array(rewards_over_time),\n",
    "            np.array(actions_over_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-21 10:10:11,732 - dqn - INFO - opinion_dqn.py:224 - Loaded configuration settings.\n",
      "2025-03-21 10:10:12,595 - dqn - INFO - opinion_dqn.py:283 - Initialized newtworks and optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:302: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(training_stats_file)\n",
      "d:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(models_load_file)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OpinionNet:\n\tsize mismatch for predict_A_b_c.weight: copying a param with shape torch.Size([18, 64]) from checkpoint, the shape in current model is torch.Size([27, 64]).\n\tsize mismatch for predict_A_b_c.bias: copying a param with shape torch.Size([18]) from checkpoint, the shape in current model is torch.Size([27]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m validation_env \u001b[38;5;241m=\u001b[39m build_environment()\n\u001b[0;32m      4\u001b[0m exp_subdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWork\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrepos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRL\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mphd-rl-algos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdqn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mopinion_dynamics\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mexperiments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2025Mar21-013722_configs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m0002_agent_params.args_.action_w_noise_amplitude_0__estimator.args_.lin_hidden_out_size_64\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_subdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      8\u001b[0m states, times, rewards, actions \u001b[38;5;241m=\u001b[39m run_policy_agent(validation_env, agent)\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36minstantiate_agent\u001b[1;34m(exp_subdir_path, train_env, validation_env)\u001b[0m\n\u001b[0;32m     44\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Instantiate the agent.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# The resume_training_path is set to the experiment folder so that the agent loads saved weights/stats.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgentDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_subdir_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# you can set this as needed\u001b[39;49;00m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msetup_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdqn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m agent\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:115\u001b[0m, in \u001b[0;36mAgentDQN.__init__\u001b[1;34m(self, train_env, validation_env, experiment_output_folder, experiment_name, resume_training_path, save_checkpoints, logger, config)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# check that all paths were provided and that the files can be found\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_training_path:\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_training_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_training_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:169\u001b[0m, in \u001b[0;36mAgentDQN.load_training_state\u001b[1;34m(self, resume_training_path)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(resume_files[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_model_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_model_file\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_model_file\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_model_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded previous training status from the following files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(resume_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    173\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\opinion_dqn.py:295\u001b[0m, in \u001b[0;36mAgentDQN.load_models\u001b[1;34m(self, models_load_file)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_models\u001b[39m(\u001b[38;5;28mself\u001b[39m, models_load_file):\n\u001b[0;32m    294\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(models_load_file)\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_model_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_model_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OpinionNet:\n\tsize mismatch for predict_A_b_c.weight: copying a param with shape torch.Size([18, 64]) from checkpoint, the shape in current model is torch.Size([27, 64]).\n\tsize mismatch for predict_A_b_c.bias: copying a param with shape torch.Size([18]) from checkpoint, the shape in current model is torch.Size([27])."
     ]
    }
   ],
   "source": [
    "train_env = build_environment()\n",
    "validation_env = build_environment()\n",
    "\n",
    "exp_subdir = r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\\2025Mar21-013722_configs\\0002_agent_params.args_.action_w_noise_amplitude_0__estimator.args_.lin_hidden_out_size_64\\1\"\n",
    "agent = instantiate_agent(exp_subdir, train_env, validation_env)\n",
    "\n",
    "num_steps = 100\n",
    "states, times, rewards, actions = run_policy_agent(validation_env, agent)\n",
    "\n",
    "plot_opinions_over_time(states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12605561, 0.2739444 , 0.        , 0.        ],\n",
       "       [0.07024676, 0.1626861 , 0.        , 0.16706714],\n",
       "       [0.12027242, 0.27972758, 0.        , 0.        ],\n",
       "       [0.07450724, 0.1610458 , 0.        , 0.16444695],\n",
       "       [0.13524365, 0.26475632, 0.        , 0.        ],\n",
       "       [0.16390488, 0.23609512, 0.        , 0.        ],\n",
       "       [0.228904  , 0.17109601, 0.        , 0.        ],\n",
       "       [0.4       , 0.        , 0.        , 0.        ],\n",
       "       [0.15238687, 0.        , 0.        , 0.24761312],\n",
       "       [0.4       , 0.        , 0.        , 0.        ],\n",
       "       [0.4       , 0.        , 0.        , 0.        ],\n",
       "       [0.09465958, 0.        , 0.        , 0.30534044],\n",
       "       [0.3282587 , 0.03587065, 0.        , 0.        ],\n",
       "       [0.27577245, 0.06211377, 0.        , 0.        ],\n",
       "       [0.22703293, 0.08648355, 0.        , 0.        ],\n",
       "       [0.070128  , 0.04021893, 0.        , 0.24943411],\n",
       "       [0.22145991, 0.08927005, 0.        , 0.        ],\n",
       "       [0.18087302, 0.10956349, 0.        , 0.        ],\n",
       "       [0.4       , 0.        , 0.        , 0.        ],\n",
       "       [0.04883672, 0.06927939, 0.        , 0.21260451],\n",
       "       [0.16287355, 0.11856323, 0.        , 0.        ],\n",
       "       [0.05652988, 0.08174185, 0.        , 0.17998642],\n",
       "       [0.02762109, 0.08968841, 0.        , 0.19300212],\n",
       "       [0.01458243, 0.09567396, 0.        , 0.19406967],\n",
       "       [0.00759942, 0.09999906, 0.        , 0.19240245],\n",
       "       [0.00351181, 0.1       , 0.        , 0.19648819],\n",
       "       [0.00071171, 0.1       , 0.        , 0.1992883 ],\n",
       "       [0.        , 0.1       , 0.        , 0.2       ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16.754519093908666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "total_value = 0\n",
    "for i, r in enumerate(rewards):\n",
    "    total_value = total_value + (gamma**i) * r\n",
    "    \n",
    "total_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
