{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chainsword\\AppData\\Local\\Temp\\ipykernel_68956\\1225861293.py:18: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def get_dir_n_levels_up(path, n):\n",
    "    # Go up n levels from the given path\n",
    "    for _ in range(n):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "proj_root = get_dir_n_levels_up(os.path.abspath(\"__file__\"), 4)\n",
    "sys.path.append(proj_root)\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dqn.opinion_dynamics.opinion_dqn import AgentDQN\n",
    "from dqn.opinion_dynamics.utils.my_logging import setup_logger\n",
    "from dqn.opinion_dynamics.utils.experiment import process_experiment\n",
    "from dqn.opinion_dynamics.utils.env_setup import EnvironmentFactory\n",
    "\n",
    "from dynamic_programming.opinion_dynamics.common.viz import plot_opinions_over_time, visualize_policy_from_env\n",
    "\n",
    "\n",
    "def instantiate_agent(exp_subdir_path: str) -> AgentDQN:\n",
    "    \"\"\"\n",
    "    Instantiate an AgentDQN using the configuration stored in a YAML file \n",
    "    in the provided experiment subdirectory. The agent is created with the \n",
    "    given training and validation environments and loads its previous state.\n",
    "    \n",
    "    Args:\n",
    "        exp_subdir_path (str): Path to the experiment subdirectory containing the config YAML and checkpoint files.\n",
    "     \n",
    "    Returns:\n",
    "        AgentDQN: An instance of AgentDQN initialized using the experiment configuration and saved state.\n",
    "    \"\"\"\n",
    "    # Assume the YAML configuration is stored as 'config.yaml' in the experiment folder.\n",
    "    config_path = os.path.join(exp_subdir_path, \"cfg.yaml\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "    \n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Instantiate the agent.\n",
    "    # The resume_training_path is set to the experiment folder so that the agent loads saved weights/stats.\n",
    "    agent = AgentDQN(\n",
    "        resume_training_path=exp_subdir_path,\n",
    "        experiment_name=config[\"experiment\"],\n",
    "        config=config,\n",
    "        save_checkpoints=False,  # you can set this as needed\n",
    "        logger=setup_logger(\"dqn\", level=logging.ERROR)\n",
    "    )\n",
    "    \n",
    "    return agent\n",
    "\n",
    "def run_policy_agent(env, agent, max_steps=10):\n",
    "    \"\"\"\n",
    "    Run the simulation using the agent’s policy (exploitation only).\n",
    "    \n",
    "    Args:\n",
    "        env: The environment (which must have a reset and step method).\n",
    "        agent: An already-trained AgentDQN instance.\n",
    "        max_steps: Maximum number of steps to run.\n",
    "        \n",
    "    Returns:\n",
    "        opinions_over_time: Array of opinions (states) over time.\n",
    "        time_points: Array of time stamps.\n",
    "        rewards_over_time: Array of rewards collected at each step.\n",
    "        actions_over_time: Array of actions taken at each step.\n",
    "    \"\"\"\n",
    "    time_points = []\n",
    "    rewards_over_time = []\n",
    "    actions_over_time = []  # New: record the actions used.\n",
    "    opinions_over_time = []\n",
    "    \n",
    "    current_time = 0.0\n",
    "    # Reset environment\n",
    "    state, _ = env.reset()\n",
    "    opinions_over_time.append(state.copy())\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Convert state to a batched tensor (batch size = 1)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        # Use the agent in exploitation mode (epsilon=0, random_action=False)\n",
    "        # The agent.select_action returns (action, beta_idx, q_value)\n",
    "        action, _, _, _ = agent.select_action(state_tensor, epsilon=agent.validation_epsilon, random_action=False, action_noise=False)\n",
    "        # action is returned as a NumPy array with shape (1, n_agents)\n",
    "        action = np.squeeze(action)  # Now action has shape (n_agents,)\n",
    "        actions_over_time.append(action.copy())\n",
    "        \n",
    "        # Apply the action in the environment.\n",
    "        next_state, reward, done, truncated, _ = env.step(action, env.tau)\n",
    "        opinions_over_time.append(next_state.copy())\n",
    "        rewards_over_time.append(reward)\n",
    "        time_points.append(current_time)\n",
    "        \n",
    "        current_time += env.tau\n",
    "        state = next_state\n",
    "        \n",
    "        if done or truncated:\n",
    "            print(f\"Simulation ended at step {step}: done={done}, truncated={truncated}\")\n",
    "            break\n",
    "\n",
    "    return (np.array(opinions_over_time),\n",
    "            np.array(time_points),\n",
    "            np.array(rewards_over_time),\n",
    "            np.array(actions_over_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\Work\\\\repos\\\\RL\\\\phd-rl-algos\\\\dqn\\\\opinion_dynamics\\\\experiments\\\\results\\\\2025Apr29-212301_configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m GAMMA = \u001b[32m0.9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Step 1: Get metadata (seed, noise, etc.) from your existing util\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m metadata_df = \u001b[43mprocess_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXPERIMENTS_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For each sub_experiment_path, select only the ones where the epoch_type is \"training\", and select only the row with the maximum frame_stamp\u001b[39;00m\n\u001b[32m      9\u001b[39m metadata_df = metadata_df[\n\u001b[32m     10\u001b[39m     (metadata_df[\u001b[33m\"\u001b[39m\u001b[33mepoch_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     & (\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     )\n\u001b[32m     15\u001b[39m ].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\utils\\experiment.py:78\u001b[39m, in \u001b[36mprocess_experiment\u001b[39m\u001b[34m(root_dir)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mprocess_experiment\u001b[39m(root_dir):\n\u001b[32m     76\u001b[39m     rows = []\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     79\u001b[39m         experiment_path = os.path.join(root_dir, name)\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(experiment_path):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'D:\\\\Work\\\\repos\\\\RL\\\\phd-rl-algos\\\\dqn\\\\opinion_dynamics\\\\experiments\\\\results\\\\2025Apr29-212301_configs'"
     ]
    }
   ],
   "source": [
    "EXP_DIR = r\"2025Apr29-212301_configs\"\n",
    "EXPERIMENTS_ROOT_DIR = r\"D:\\Work\\repos\\RL\\phd-rl-algos\\dqn\\opinion_dynamics\\experiments\\results\"\n",
    "EXPERIMENTS_ROOT = os.path.join(EXPERIMENTS_ROOT_DIR, EXP_DIR)\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Step 1: Get metadata (seed, noise, etc.) from your existing util\n",
    "metadata_df = process_experiment(EXPERIMENTS_ROOT)\n",
    "# For each sub_experiment_path, select only the ones where the epoch_type is \"training\", and select only the row with the maximum frame_stamp\n",
    "metadata_df = metadata_df[\n",
    "    (metadata_df[\"epoch_type\"] == \"training\")\n",
    "    & (\n",
    "        metadata_df[\"frame_stamp\"]\n",
    "        == metadata_df.groupby(\"sub_experiment_path\")[\"frame_stamp\"].transform(max)\n",
    "    )\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Step 2: Run agents and gather reward trajectories\n",
    "all_runs = []\n",
    "\n",
    "for i, row in metadata_df.iterrows():\n",
    "    print(\n",
    "        f\"Running experiment number {i+1} out of {len(metadata_df)}: {row['sub_experiment_path']}\"\n",
    "    )\n",
    "    subdir = row[\"sub_experiment_path\"]\n",
    "    noise = float(row.get(\"sub_exp_cfg_agent_params.args_.action_w_noise_amplitude\", 0))\n",
    "    seed = row.get(\"seed\", \"unknown\")\n",
    "\n",
    "    try:\n",
    "        env_factory = EnvironmentFactory()\n",
    "        env = env_factory.get_validation_env(version=0)\n",
    "        agent = instantiate_agent(subdir)\n",
    "        opinions, times, rewards, actions = run_policy_agent(env, agent, max_steps=30)\n",
    "\n",
    "        discounted_return = sum((GAMMA**t) * r for t, r in enumerate(rewards))\n",
    "\n",
    "        for step, (t, opinion_vec) in enumerate(zip(times, opinions)):\n",
    "            for agent_id, opinion_val in enumerate(opinion_vec):\n",
    "                all_runs.append(\n",
    "                    {\n",
    "                        \"noise_amplitude\": noise,\n",
    "                        \"seed\": seed,\n",
    "                        \"time\": t,\n",
    "                        \"step\": step,\n",
    "                        \"agent_id\": agent_id,\n",
    "                        \"opinion\": opinion_val,\n",
    "                        \"experiment\": os.path.basename(subdir),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to run experiment at {subdir}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Step 3: Plot\n",
    "df = pd.DataFrame(all_runs)\n",
    "df[\"seed\"] = df[\"seed\"].astype(str)\n",
    "df[\"noise_amplitude\"] = df[\"noise_amplitude\"].astype(str)\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df,\n",
    "    row=\"seed\",\n",
    "    col=\"noise_amplitude\",\n",
    "    hue=\"agent_id\",\n",
    "    margin_titles=True,\n",
    "    sharey=True,\n",
    ")\n",
    "g.map_dataframe(sns.lineplot, x=\"step\", y=\"opinion\")\n",
    "g.set_axis_labels(\"Time\", \"Opinion\")\n",
    "g.set_titles(row_template=\"Seed: {row_name}\", col_template=\"Noise: {col_name}\")\n",
    "g.add_legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noise_amplitude</th>\n",
       "      <th>seed</th>\n",
       "      <th>time</th>\n",
       "      <th>step</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>opinion</th>\n",
       "      <th>experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.385480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2120</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.888052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2124 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     noise_amplitude seed  time  step  agent_id   opinion experiment\n",
       "0                0.0    0   0.0     0         0  0.300000          0\n",
       "1                0.0    0   0.0     0         1  0.200000          0\n",
       "2                0.0    0   0.0     0         2  0.100000          0\n",
       "3                0.0    0   0.0     0         3  0.000000          0\n",
       "4                0.0    0   0.1     1         0  0.385480          0\n",
       "...              ...  ...   ...   ...       ...       ...        ...\n",
       "2119             0.3    1   1.6    16         3  0.872200          1\n",
       "2120             0.3    1   1.7    17         0  0.974237          1\n",
       "2121             0.3    1   1.7    17         1  0.967567          1\n",
       "2122             0.3    1   1.7    17         2  0.950695          1\n",
       "2123             0.3    1   1.7    17         3  0.888052          1\n",
       "\n",
       "[2124 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
