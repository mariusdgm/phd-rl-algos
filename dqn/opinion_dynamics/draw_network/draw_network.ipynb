{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionNetCommonAB(nn.Module):\n",
    "    def __init__(self, nr_agents, nr_betas=2, lin_hidden_size=64):\n",
    "        super(OpinionNetCommonAB, self).__init__()\n",
    "\n",
    "        self.nr_agents = nr_agents\n",
    "        self.nr_betas = nr_betas  # Number of \\(\\beta\\) grid points\n",
    "        self.lin_hidden_size = lin_hidden_size\n",
    "\n",
    "        # Fully connected layers for state features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.nr_agents, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.lin_hidden_size, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.predict_shared_A_b = nn.Linear(self.lin_hidden_size, 2 * self.nr_agents)\n",
    "        self.predict_c = nn.Linear(self.lin_hidden_size, self.nr_betas)\n",
    "\n",
    "        # Optional: initialize c_j biases to zero\n",
    "        with torch.no_grad():\n",
    "            self.predict_c.bias.zero_()\n",
    "\n",
    "    def forward(self, x, w=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input state features, shape (B, N)\n",
    "            w (torch.Tensor, optional): Optional action vector for each sample, shape (B, N)\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - A_diag (torch.Tensor): shape (B, J, N), positive definite diagonals of A\n",
    "                - b (torch.Tensor): shape (B, J, N), linear coefficients\n",
    "                - c (torch.Tensor): shape (B, J), bias term\n",
    "                - q (torch.Tensor): shape (B, J), Q-values (only if w is provided)\n",
    "        \"\"\"\n",
    "        features = self.fc(x)\n",
    "\n",
    "        # Shared A and b\n",
    "        A_b_shared = self.predict_shared_A_b(features)  # (B, 2N)\n",
    "        A_diag = F.softplus(A_b_shared[:, :self.nr_agents]) + 1e-6  # (B, N)\n",
    "        b = A_b_shared[:, self.nr_agents:]  # (B, N)\n",
    "\n",
    "        # Repeat across betas\n",
    "        A_diag = A_diag.unsqueeze(1).repeat(1, self.nr_betas, 1)  # (B, J, N)\n",
    "        b = b.unsqueeze(1).repeat(1, self.nr_betas, 1)            # (B, J, N)\n",
    "\n",
    "        # Independent c values for each beta\n",
    "        c = self.predict_c(features)  # (B, J)\n",
    "        c = torch.tanh(c) * 100.0 # Bounding to help with stability\n",
    "        \n",
    "        # print(f\"A_diag shape: {A_diag.shape}, b shape: {b.shape}, c shape: {c.shape}\")\n",
    "        \n",
    "        output = {\"A_diag\": A_diag, \"b\": b, \"c\": c}\n",
    "\n",
    "        # We are braodcasting w over all J levels here\n",
    "        if w is not None:\n",
    "            w = w.unsqueeze(1)  # (B, 1, N)\n",
    "            q = self.compute_q_values(w, A_diag, b, c)  # (B, J)\n",
    "            output[\"q\"] = q\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_w_star(A_diag, b):\n",
    "        \"\"\"\n",
    "        Compute the optimal weight allocation \\( w^* \\) given A_diag and b.\n",
    "\n",
    "        Args:\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Optimal weight allocation \\( w^* \\), shape (batch_size, nr_betas, nr_agents).\n",
    "        \"\"\"\n",
    "        A_inv = 1.0 / A_diag  # Inverse of diagonal A\n",
    "        w = A_inv * b  # Compute raw w*\n",
    "\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_q_values(w, A_diag, b, c):\n",
    "        \"\"\"\n",
    "        Compute Q-values using the optimal weight allocation w*.\n",
    "\n",
    "        Args:\n",
    "            w_star (torch.Tensor): Optimal weight allocation, shape (batch_size, nr_betas, nr_agents).\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "            c (torch.Tensor): Free term, shape (batch_size, nr_betas).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Q-values, shape (batch_size, nr_betas).\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            w.shape == A_diag.shape == b.shape\n",
    "        ), f\"Shape mismatch: w_star={w.shape}, A_diag={A_diag.shape}, b={b.shape}\"\n",
    "\n",
    "        # Quadratic term: w^T A w = sum_i A_i * w_i^2\n",
    "        quadratic_term = 0.5 * (A_diag * w.pow(2)).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Linear term: b^T w\n",
    "        linear_term = (b * w).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Total Q-value\n",
    "        q_values = c - quadratic_term + linear_term  # shape (B, J)\n",
    "        \n",
    "        assert q_values.shape == (w.shape[0], w.shape[1]), \\\n",
    "            f\"Expected shape (B, J), got {q_values.shape}\"\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_action_noise(w, noise_amplitude):\n",
    "        \"\"\"\n",
    "        Add noise to the weight vector w.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): The optimal weight vector computed from the network.\n",
    "            noise_amplitude (float): The scale of the Gaussian noise.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The noisy, normalized weight vector.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(w) * noise_amplitude\n",
    "        noisy_w = w + noise\n",
    "\n",
    "        return noisy_w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_action_from_w(w: torch.Tensor, beta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the action u from allocation weights w and beta values.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): Allocation weights, shape (batch_size, num_agents)\n",
    "            beta (torch.Tensor): Per-agent beta values, shape (batch_size, num_agents)\n",
    "            max_u (float): Maximum action value\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Actions u, shape (batch_size, num_agents), capped at max_u per agent\n",
    "        \"\"\"\n",
    "        # Softmax also normalizes\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        u = w * beta \n",
    "        return u\n",
    "    \n",
    "class OpinionNet(nn.Module):\n",
    "    def __init__(self, nr_agents, nr_betas=2, lin_hidden_size=64):\n",
    "        super(OpinionNet, self).__init__()\n",
    "\n",
    "        self.nr_agents = nr_agents\n",
    "        self.nr_betas = nr_betas  # Number of \\(\\beta\\) grid points\n",
    "        self.lin_hidden_size = lin_hidden_size\n",
    "\n",
    "        # Fully connected layers for state features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.nr_agents, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.lin_hidden_size, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Predict \\( q(x, \\beta; \\theta), A, b \\) for all \\(\\beta\\) grid points\n",
    "        self.predict_A_b_c = nn.Linear(\n",
    "            self.lin_hidden_size, self.nr_betas * (2 * self.nr_agents + 1)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            full_bias = self.predict_A_b_c.bias  # shape: (nr_betas * (2*n + 1),)\n",
    "            block_size = 2 * self.nr_agents + 1\n",
    "            for j in range(self.nr_betas):\n",
    "                full_bias[j * block_size] = 0.0  # Initialize c_j to 0.\n",
    "\n",
    "    def forward(self, x, w=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input state features, shape (B, N)\n",
    "            w (torch.Tensor, optional): Optional action vector for each sample, shape (B, N)\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - A_diag (torch.Tensor): shape (B, J, N), positive definite diagonals of A\n",
    "                - b (torch.Tensor): shape (B, J, N), linear coefficients\n",
    "                - c (torch.Tensor): shape (B, J), bias term\n",
    "                - q (torch.Tensor): shape (B, J), Q-values (only if w is provided)\n",
    "        \"\"\"\n",
    "        features = self.fc(x)\n",
    "\n",
    "        A_b_c_net = self.predict_A_b_c(features)\n",
    "        A_b_c_net = A_b_c_net.reshape(-1, self.nr_betas, 2 * self.nr_agents + 1)\n",
    "\n",
    "        A_diag = F.softplus(A_b_c_net[:, :, 1 : self.nr_agents + 1]) + 1e-6  # (B, J, N)\n",
    "        b = A_b_c_net[:, :, self.nr_agents + 1 :]  # (B, J, N)\n",
    "        c = A_b_c_net[:, :, 0]  # (B, J)\n",
    "\n",
    "        # print(f\"A_diag shape: {A_diag.shape}, b shape: {b.shape}, c shape: {c.shape}\")\n",
    "        \n",
    "        output = {\"A_diag\": A_diag, \"b\": b, \"c\": c}\n",
    "\n",
    "        # We are braodcasting w over all J levels here\n",
    "        if w is not None:\n",
    "            w = w.unsqueeze(1)  # (B, 1, N)\n",
    "            q = self.compute_q_values(w, A_diag, b, c)  # (B, J)\n",
    "            output[\"q\"] = q\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_w_star(A_diag, b):\n",
    "        \"\"\"\n",
    "        Compute the optimal weight allocation \\( w^* \\) given A_diag and b.\n",
    "\n",
    "        Args:\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Optimal weight allocation \\( w^* \\), shape (batch_size, nr_betas, nr_agents).\n",
    "        \"\"\"\n",
    "        A_inv = 1.0 / A_diag  # Inverse of diagonal A\n",
    "        w = A_inv * b  # Compute raw w*\n",
    "\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_q_values(w, A_diag, b, c):\n",
    "        \"\"\"\n",
    "        Compute Q-values using the optimal weight allocation w*.\n",
    "\n",
    "        Args:\n",
    "            w_star (torch.Tensor): Optimal weight allocation, shape (batch_size, nr_betas, nr_agents).\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "            c (torch.Tensor): Free term, shape (batch_size, nr_betas).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Q-values, shape (batch_size, nr_betas).\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            w.shape == A_diag.shape == b.shape\n",
    "        ), f\"Shape mismatch: w_star={w.shape}, A_diag={A_diag.shape}, b={b.shape}\"\n",
    "\n",
    "        # Quadratic term: w^T A w = sum_i A_i * w_i^2\n",
    "        quadratic_term = 0.5 * (A_diag * w.pow(2)).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Linear term: b^T w\n",
    "        linear_term = (b * w).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Total Q-value\n",
    "        q_values = c - quadratic_term + linear_term  # shape (B, J)\n",
    "        \n",
    "        assert q_values.shape == (w.shape[0], w.shape[1]), \\\n",
    "            f\"Expected shape (B, J), got {q_values.shape}\"\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_action_noise(w, noise_amplitude):\n",
    "        \"\"\"\n",
    "        Add noise to the weight vector w.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): The optimal weight vector computed from the network.\n",
    "            noise_amplitude (float): The scale of the Gaussian noise.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The noisy, normalized weight vector.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(w) * noise_amplitude\n",
    "        noisy_w = w + noise\n",
    "\n",
    "        return noisy_w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_action_from_w(w: torch.Tensor, beta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the action u from allocation weights w and beta values.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): Allocation weights, shape (batch_size, num_agents)\n",
    "            beta (torch.Tensor): Per-agent beta values, shape (batch_size, num_agents)\n",
    "            max_u (float): Maximum action value\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Actions u, shape (batch_size, num_agents), capped at max_u per agent\n",
    "        \"\"\"\n",
    "        # Softmax also normalizes\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        u = w * beta \n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nr_agents = 20\n",
    "# model = OpinionNet(nr_agents, \n",
    "#                    nr_betas=1, \n",
    "#                    lin_hidden_size=64)\n",
    "\n",
    "# dummy_input = torch.randn(1, nr_agents)  # Correct shape: (batch_size=1, num_agents=20)\n",
    "\n",
    "\n",
    "# # Export to ONNX\n",
    "# torch.onnx.export(\n",
    "#     model, \n",
    "#     dummy_input, \n",
    "#     \"OpinionNet_model.onnx\",\n",
    "#     input_names=[\"input\"],\n",
    "#     output_names=[\"output\"],\n",
    "#     opset_version=11\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:56588): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opinionnet_simplified.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import torch\n",
    "\n",
    "model = OpinionNet(nr_agents=20, nr_betas=3, lin_hidden_size=64).eval()\n",
    "# depth=1 collapses internals; increase to 2 if you want to peek into the FC stack\n",
    "graph = draw_graph(\n",
    "    model,\n",
    "    input_size=(1, 20),                 # (batch, features)\n",
    "    expand_nested=False,                # keep submodules as one box\n",
    "    depth=1,\n",
    "    graph_name=\"OpinionNet (simplified)\",\n",
    "    roll=True,                          # roll identical modules\n",
    ")\n",
    "graph.visual_graph.render(\"opinionnet_simplified\", format=\"png\", cleanup=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:20060): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'opinionnet_commonab_simplified.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpinionNetCommonAB(nr_agents=20, nr_betas=3, lin_hidden_size=64).eval()\n",
    "graph = draw_graph(\n",
    "    model,\n",
    "    input_size=(1, 20),\n",
    "    expand_nested=False,\n",
    "    depth=1,\n",
    "    graph_name=\"OpinionNetCommonAB (simplified)\",\n",
    "    roll=True,\n",
    ")\n",
    "graph.visual_graph.render(\"opinionnet_commonab_simplified\", format=\"png\", cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### More control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opinionnet_slim.png'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from graphviz import Digraph\n",
    "\n",
    "nr_agents, nr_betas, lin_hidden_size = 20, 3, 64\n",
    "model = OpinionNet(nr_agents, nr_betas, lin_hidden_size).eval()\n",
    "\n",
    "# Grab shapes once (handy for labels)\n",
    "_ = summary(model, input_size=(1, nr_agents), depth=2, verbose=0)\n",
    "\n",
    "g = Digraph(\"OpinionNet_slim\", format=\"png\")\n",
    "g.attr(rankdir=\"LR\", fontsize=\"12\")\n",
    "\n",
    "g.node(\"x\", f\"Input x\\n(B, {nr_agents})\", shape=\"box\")\n",
    "g.node(\"FC\", f\"FC block\\nLinear({nr_agents}->{lin_hidden_size})\\nReLU\\nLinear({lin_hidden_size}->{lin_hidden_size})\\nReLU\", shape=\"box\")\n",
    "g.node(\"Head\", f\"Head: A_diag, b, c\\nShapes:\\nA_diag (B, J, N)\\nb (B, J, N)\\nc (B, J)\\nJ={nr_betas}, N={nr_agents}\", shape=\"box\")\n",
    "\n",
    "g.edge(\"x\", \"FC\")\n",
    "g.edge(\"FC\", \"Head\")\n",
    "g.render(\"opinionnet_slim\", cleanup=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
