{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "import hiddenlayer as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpinionNet(nn.Module):\n",
    "    def __init__(self, nr_agents, nr_betas=2, lin_hidden_size=64):\n",
    "        super(OpinionNet, self).__init__()\n",
    "\n",
    "        self.nr_agents = nr_agents\n",
    "        self.nr_betas = nr_betas  # Number of \\(\\beta\\) grid points\n",
    "        self.lin_hidden_size = lin_hidden_size\n",
    "\n",
    "        # Fully connected layers for state features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.nr_agents, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.lin_hidden_size, self.lin_hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.predict_shared_A_b = nn.Linear(self.lin_hidden_size, 2 * self.nr_agents)\n",
    "        self.predict_c = nn.Linear(self.lin_hidden_size, self.nr_betas)\n",
    "\n",
    "        # Optional: initialize c_j biases to zero\n",
    "        with torch.no_grad():\n",
    "            self.predict_c.bias.zero_()\n",
    "\n",
    "    def forward(self, x, w=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input state features, shape (B, N)\n",
    "            w (torch.Tensor, optional): Optional action vector for each sample, shape (B, N)\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - A_diag (torch.Tensor): shape (B, J, N), positive definite diagonals of A\n",
    "                - b (torch.Tensor): shape (B, J, N), linear coefficients\n",
    "                - c (torch.Tensor): shape (B, J), bias term\n",
    "                - q (torch.Tensor): shape (B, J), Q-values (only if w is provided)\n",
    "        \"\"\"\n",
    "        features = self.fc(x)\n",
    "\n",
    "        # Shared A and b\n",
    "        A_b_shared = self.predict_shared_A_b(features)  # (B, 2N)\n",
    "        A_diag = F.softplus(A_b_shared[:, :self.nr_agents]) + 1e-6  # (B, N)\n",
    "        b = A_b_shared[:, self.nr_agents:]  # (B, N)\n",
    "\n",
    "        # Repeat across betas\n",
    "        A_diag = A_diag.unsqueeze(1).repeat(1, self.nr_betas, 1)  # (B, J, N)\n",
    "        b = b.unsqueeze(1).repeat(1, self.nr_betas, 1)            # (B, J, N)\n",
    "\n",
    "        # Independent c values for each beta\n",
    "        c = self.predict_c(features)  # (B, J)\n",
    "\n",
    "        # print(f\"A_diag shape: {A_diag.shape}, b shape: {b.shape}, c shape: {c.shape}\")\n",
    "        \n",
    "        output = {\"A_diag\": A_diag, \"b\": b, \"c\": c}\n",
    "\n",
    "        # We are braodcasting w over all J levels here\n",
    "        if w is not None:\n",
    "            w = w.unsqueeze(1)  # (B, 1, N)\n",
    "            q = self.compute_q_values(w, A_diag, b, c)  # (B, J)\n",
    "            output[\"q\"] = q\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_w_star(A_diag, b):\n",
    "        \"\"\"\n",
    "        Compute the optimal weight allocation \\( w^* \\) given A_diag and b.\n",
    "\n",
    "        Args:\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Optimal weight allocation \\( w^* \\), shape (batch_size, nr_betas, nr_agents).\n",
    "        \"\"\"\n",
    "        A_inv = 1.0 / A_diag  # Inverse of diagonal A\n",
    "        w = A_inv * b  # Compute raw w*\n",
    "\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_q_values(w, A_diag, b, c):\n",
    "        \"\"\"\n",
    "        Compute Q-values using the optimal weight allocation w*.\n",
    "\n",
    "        Args:\n",
    "            w_star (torch.Tensor): Optimal weight allocation, shape (batch_size, nr_betas, nr_agents).\n",
    "            A_diag (torch.Tensor): Diagonal elements of A, shape (batch_size, nr_betas, nr_agents).\n",
    "            b (torch.Tensor): Bias term, shape (batch_size, nr_betas, nr_agents).\n",
    "            c (torch.Tensor): Free term, shape (batch_size, nr_betas).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Computed Q-values, shape (batch_size, nr_betas).\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            w.shape == A_diag.shape == b.shape\n",
    "        ), f\"Shape mismatch: w_star={w.shape}, A_diag={A_diag.shape}, b={b.shape}\"\n",
    "\n",
    "        # Quadratic term: w^T A w = sum_i A_i * w_i^2\n",
    "        quadratic_term = 0.5 * (A_diag * w.pow(2)).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Linear term: b^T w\n",
    "        linear_term = (b * w).sum(dim=2)  # shape (B, J)\n",
    "\n",
    "        # Total Q-value\n",
    "        q_values = c - quadratic_term + linear_term  # shape (B, J)\n",
    "        \n",
    "        assert q_values.shape == (w.shape[0], w.shape[1]), \\\n",
    "            f\"Expected shape (B, J), got {q_values.shape}\"\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_action_noise(w, noise_amplitude):\n",
    "        \"\"\"\n",
    "        Add noise to the weight vector w.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): The optimal weight vector computed from the network.\n",
    "            noise_amplitude (float): The scale of the Gaussian noise.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The noisy, normalized weight vector.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(w) * noise_amplitude\n",
    "        noisy_w = w + noise\n",
    "\n",
    "        return noisy_w\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_action_from_w(w: torch.Tensor, beta: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the action u from allocation weights w and beta values.\n",
    "\n",
    "        Args:\n",
    "            w (torch.Tensor): Allocation weights, shape (batch_size, num_agents)\n",
    "            beta (torch.Tensor): Per-agent beta values, shape (batch_size, num_agents)\n",
    "            max_u (float): Maximum action value\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Actions u, shape (batch_size, num_agents), capped at max_u per agent\n",
    "        \"\"\"\n",
    "        # Softmax also normalizes\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        u = w * beta \n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_agents = 20\n",
    "model = OpinionNet(nr_agents, \n",
    "                   nr_betas=1, \n",
    "                   lin_hidden_size=64)\n",
    "\n",
    "dummy_input = torch.randn(1, nr_agents)  # Correct shape: (batch_size=1, num_agents=20)\n",
    "\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"OpinionNet_model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
